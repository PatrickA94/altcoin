{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge to single Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "### Check for missing data\n",
    "### Write function that evaluates each model for each currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/home/patrick/Documents/Alt Coin Proj/data/CombinedData.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['BTC','USDT','PASC','GNT','GNO'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "colarr = []\n",
    "for col in df.columns.unique():\n",
    "    colarr.append(col[0])\n",
    "colarr = pd.Series(colarr)\n",
    "colarr = colarr.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOGE Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.67      1.00      0.80       834\n",
      "        1.0       0.00      0.00      0.00       416\n",
      "\n",
      "avg / total       0.45      0.67      0.53      1250\n",
      "\n",
      "[[834   0]\n",
      " [416   0]]\n",
      "NAV Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      1.00      0.65       597\n",
      "        1.0       1.00      0.01      0.02       653\n",
      "\n",
      "avg / total       0.75      0.48      0.32      1250\n",
      "\n",
      "[[597   0]\n",
      " [648   5]]\n",
      "NMC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.66      0.56       600\n",
      "        1.0       0.52      0.33      0.41       650\n",
      "\n",
      "avg / total       0.50      0.49      0.48      1250\n",
      "\n",
      "[[397 203]\n",
      " [433 217]]\n",
      "NXT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       612\n",
      "        1.0       0.51      1.00      0.68       638\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1250\n",
      "\n",
      "[[  0 612]\n",
      " [  0 638]]\n",
      "DCR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.02      0.04       625\n",
      "        1.0       0.50      0.98      0.66       625\n",
      "\n",
      "avg / total       0.52      0.50      0.35      1250\n",
      "\n",
      "[[ 14 611]\n",
      " [ 12 613]]\n",
      "FLO Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       612\n",
      "        1.0       0.51      1.00      0.68       638\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1250\n",
      "\n",
      "[[  0 612]\n",
      " [  0 638]]\n",
      "REP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       613\n",
      "        1.0       0.51      1.00      0.67       637\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1250\n",
      "\n",
      "[[  0 613]\n",
      " [  2 635]]\n",
      "BCY Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.85      0.63       620\n",
      "        1.0       0.54      0.17      0.26       630\n",
      "\n",
      "avg / total       0.52      0.51      0.45      1250\n",
      "\n",
      "[[529  91]\n",
      " [522 108]]\n",
      "FCT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.39      0.43       622\n",
      "        1.0       0.49      0.59      0.54       628\n",
      "\n",
      "avg / total       0.49      0.49      0.48      1250\n",
      "\n",
      "[[241 381]\n",
      " [258 370]]\n",
      "XRP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       608\n",
      "        1.0       0.51      1.00      0.68       642\n",
      "\n",
      "avg / total       0.26      0.51      0.35      1250\n",
      "\n",
      "[[  0 608]\n",
      " [  0 642]]\n",
      "CLAM Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.85      0.63       621\n",
      "        1.0       0.52      0.16      0.25       624\n",
      "\n",
      "avg / total       0.51      0.50      0.44      1245\n",
      "\n",
      "[[525  96]\n",
      " [522 102]]\n",
      "STRAT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       630\n",
      "        1.0       0.50      1.00      0.66       620\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1250\n",
      "\n",
      "[[  0 630]\n",
      " [  0 620]]\n",
      "NAUT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.72      0.59       621\n",
      "        1.0       0.50      0.28      0.36       629\n",
      "\n",
      "avg / total       0.50      0.50      0.47      1250\n",
      "\n",
      "[[445 176]\n",
      " [450 179]]\n",
      "MAID Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.70      0.60       642\n",
      "        1.0       0.51      0.34      0.41       608\n",
      "\n",
      "avg / total       0.52      0.52      0.51      1250\n",
      "\n",
      "[[449 193]\n",
      " [404 204]]\n",
      "PPC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.33      0.39       594\n",
      "        1.0       0.52      0.66      0.58       656\n",
      "\n",
      "avg / total       0.50      0.51      0.49      1250\n",
      "\n",
      "[[197 397]\n",
      " [221 435]]\n",
      "VIA Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.63      0.54       601\n",
      "        1.0       0.51      0.35      0.42       649\n",
      "\n",
      "avg / total       0.49      0.49      0.48      1250\n",
      "\n",
      "[[381 220]\n",
      " [420 229]]\n",
      "XCP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.84      0.62       615\n",
      "        1.0       0.47      0.14      0.22       635\n",
      "\n",
      "avg / total       0.48      0.48      0.41      1250\n",
      "\n",
      "[[516  99]\n",
      " [546  89]]\n",
      "XPM Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.45      0.30      0.36       610\n",
      "        1.0       0.50      0.66      0.57       640\n",
      "\n",
      "avg / total       0.48      0.48      0.47      1250\n",
      "\n",
      "[[182 428]\n",
      " [219 421]]\n",
      "ETH Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.36      0.43       635\n",
      "        1.0       0.51      0.68      0.58       615\n",
      "\n",
      "avg / total       0.52      0.52      0.51      1250\n",
      "\n",
      "[[229 406]\n",
      " [197 418]]\n",
      "DASH Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.00      0.01       647\n",
      "        1.0       0.48      1.00      0.65       603\n",
      "\n",
      "avg / total       0.54      0.48      0.32      1250\n",
      "\n",
      "[[  3 644]\n",
      " [  2 601]]\n",
      "BTS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      1.00      0.68       647\n",
      "        1.0       0.00      0.00      0.00       603\n",
      "\n",
      "avg / total       0.27      0.52      0.35      1250\n",
      "\n",
      "[[647   0]\n",
      " [603   0]]\n",
      "SYS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       618\n",
      "        1.0       0.51      1.00      0.67       632\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1250\n",
      "\n",
      "[[  0 618]\n",
      " [  0 632]]\n",
      "NOTE Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.72      0.60       644\n",
      "        1.0       0.49      0.28      0.36       606\n",
      "\n",
      "avg / total       0.50      0.51      0.48      1250\n",
      "\n",
      "[[465 179]\n",
      " [436 170]]\n",
      "FLDC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.59      0.65      0.62       692\n",
      "        1.0       0.51      0.45      0.48       558\n",
      "\n",
      "avg / total       0.56      0.56      0.56      1250\n",
      "\n",
      "[[451 241]\n",
      " [308 250]]\n",
      "EMC2 Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.59      1.00      0.74       741\n",
      "        1.0       0.00      0.00      0.00       509\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1250\n",
      "\n",
      "[[741   0]\n",
      " [509   0]]\n",
      "SC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      1.00      0.75       746\n",
      "        1.0       0.00      0.00      0.00       499\n",
      "\n",
      "avg / total       0.36      0.60      0.45      1245\n",
      "\n",
      "[[746   0]\n",
      " [499   0]]\n",
      "PINK Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       683\n",
      "        1.0       0.45      1.00      0.62       567\n",
      "\n",
      "avg / total       0.21      0.45      0.28      1250\n",
      "\n",
      "[[  0 683]\n",
      " [  1 566]]\n",
      "XMR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.83      0.63       616\n",
      "        1.0       0.55      0.21      0.30       634\n",
      "\n",
      "avg / total       0.53      0.51      0.46      1250\n",
      "\n",
      "[[511 105]\n",
      " [504 130]]\n",
      "HUC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.75      0.60       595\n",
      "        1.0       0.59      0.33      0.43       655\n",
      "\n",
      "avg / total       0.55      0.53      0.51      1250\n",
      "\n",
      "[[445 150]\n",
      " [436 219]]\n",
      "SJCX Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.82      0.61       603\n",
      "        1.0       0.50      0.16      0.24       647\n",
      "\n",
      "avg / total       0.49      0.48      0.42      1250\n",
      "\n",
      "[[497 106]\n",
      " [542 105]]\n",
      "XBC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.90      0.66       625\n",
      "        1.0       0.60      0.14      0.23       625\n",
      "\n",
      "avg / total       0.56      0.52      0.44      1250\n",
      "\n",
      "[[565  60]\n",
      " [535  90]]\n",
      "STR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      1.00      0.68       650\n",
      "        1.0       0.00      0.00      0.00       600\n",
      "\n",
      "avg / total       0.27      0.52      0.36      1250\n",
      "\n",
      "[[650   0]\n",
      " [600   0]]\n",
      "DGB Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.63      1.00      0.77       782\n",
      "        1.0       0.00      0.00      0.00       468\n",
      "\n",
      "avg / total       0.39      0.63      0.48      1250\n",
      "\n",
      "[[782   0]\n",
      " [468   0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLK Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.80      0.63       632\n",
      "        1.0       0.53      0.23      0.32       618\n",
      "\n",
      "avg / total       0.52      0.52      0.48      1250\n",
      "\n",
      "[[504 128]\n",
      " [475 143]]\n",
      "ZEC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.06      0.10       558\n",
      "        1.0       0.55      0.95      0.70       692\n",
      "\n",
      "avg / total       0.52      0.55      0.43      1250\n",
      "\n",
      "[[ 32 526]\n",
      " [ 36 656]]\n",
      "NXC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      1.00      0.66       617\n",
      "        1.0       0.00      0.00      0.00       633\n",
      "\n",
      "avg / total       0.24      0.49      0.33      1250\n",
      "\n",
      "[[617   0]\n",
      " [633   0]]\n",
      "POT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       622\n",
      "        1.0       0.50      1.00      0.67       628\n",
      "\n",
      "avg / total       0.25      0.50      0.34      1250\n",
      "\n",
      "[[  0 622]\n",
      " [  0 628]]\n",
      "LSK Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.75      0.59       609\n",
      "        1.0       0.53      0.27      0.36       641\n",
      "\n",
      "avg / total       0.51      0.50      0.47      1250\n",
      "\n",
      "[[456 153]\n",
      " [469 172]]\n",
      "GAME Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.46      0.26      0.34       604\n",
      "        1.0       0.51      0.71      0.59       646\n",
      "\n",
      "avg / total       0.49      0.50      0.47      1250\n",
      "\n",
      "[[159 445]\n",
      " [185 461]]\n",
      "BTCD Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.76      0.63       650\n",
      "        1.0       0.52      0.29      0.37       600\n",
      "\n",
      "avg / total       0.53      0.53      0.50      1250\n",
      "\n",
      "[[493 157]\n",
      " [428 172]]\n",
      "VRC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.84      0.63       612\n",
      "        1.0       0.60      0.23      0.34       638\n",
      "\n",
      "avg / total       0.56      0.53      0.48      1250\n",
      "\n",
      "[[512 100]\n",
      " [489 149]]\n",
      "LBC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       609\n",
      "        1.0       0.51      1.00      0.68       641\n",
      "\n",
      "avg / total       0.26      0.51      0.35      1250\n",
      "\n",
      "[[  0 609]\n",
      " [  0 641]]\n",
      "NEOS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.98      0.65       606\n",
      "        1.0       0.55      0.03      0.05       644\n",
      "\n",
      "avg / total       0.52      0.49      0.34      1250\n",
      "\n",
      "[[591  15]\n",
      " [626  18]]\n",
      "LTC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.55      0.51       572\n",
      "        1.0       0.55      0.47      0.51       673\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1245\n",
      "\n",
      "[[317 255]\n",
      " [358 315]]\n",
      "VTC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       617\n",
      "        1.0       0.51      1.00      0.67       633\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1250\n",
      "\n",
      "[[  0 617]\n",
      " [  0 633]]\n",
      "OMNI Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.95      0.66       632\n",
      "        1.0       0.52      0.05      0.10       618\n",
      "\n",
      "avg / total       0.51      0.51      0.38      1250\n",
      "\n",
      "[[601  31]\n",
      " [585  33]]\n",
      "GRC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.87      0.65       636\n",
      "        1.0       0.52      0.15      0.23       614\n",
      "\n",
      "avg / total       0.52      0.52      0.44      1250\n",
      "\n",
      "[[554  82]\n",
      " [524  90]]\n",
      "EXP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       611\n",
      "        1.0       0.51      1.00      0.68       639\n",
      "\n",
      "avg / total       0.26      0.51      0.35      1250\n",
      "\n",
      "[[  0 611]\n",
      " [  0 639]]\n",
      "XEM Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      1.00      0.69       659\n",
      "        1.0       0.00      0.00      0.00       591\n",
      "\n",
      "avg / total       0.28      0.53      0.36      1250\n",
      "\n",
      "[[659   0]\n",
      " [591   0]]\n",
      "RADS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.39      0.44       598\n",
      "        1.0       0.54      0.64      0.59       652\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1250\n",
      "\n",
      "[[235 363]\n",
      " [232 420]]\n",
      "ETC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.54      0.50       591\n",
      "        1.0       0.52      0.46      0.49       659\n",
      "\n",
      "avg / total       0.50      0.49      0.49      1250\n",
      "\n",
      "[[318 273]\n",
      " [359 300]]\n",
      "ARDR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       630\n",
      "        1.0       0.50      1.00      0.66       620\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1250\n",
      "\n",
      "[[  0 630]\n",
      " [  0 620]]\n",
      "BELA Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.75      0.61       630\n",
      "        1.0       0.52      0.27      0.36       620\n",
      "\n",
      "avg / total       0.52      0.51      0.49      1250\n",
      "\n",
      "[[473 157]\n",
      " [450 170]]\n",
      "AMP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       611\n",
      "        1.0       0.51      1.00      0.68       639\n",
      "\n",
      "avg / total       0.26      0.51      0.35      1250\n",
      "\n",
      "[[  0 611]\n",
      " [  0 639]]\n"
     ]
    }
   ],
   "source": [
    "for coin in colarr:\n",
    "    crypt = df[coin]\n",
    "    trainX = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00'].drop('up',axis=1)\n",
    "    trainY = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00']['up']\n",
    "    testX = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()].drop('up',axis=1)\n",
    "    testY = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()]['up']\n",
    "    \n",
    "    print coin + ' Logistic Regression'\n",
    "    \n",
    "    logmodel = LogisticRegression()\n",
    "    logmodel.fit(trainX,trainY)\n",
    "    pred = logmodel.predict(testX)\n",
    "    print classification_report(testY,pred)\n",
    "    print confusion_matrix(testY,pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOGE Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.67      1.00      0.80       834\n",
      "        1.0       0.00      0.00      0.00       416\n",
      "\n",
      "avg / total       0.45      0.67      0.53      1250\n",
      "\n",
      "[[834   0]\n",
      " [416   0]]\n",
      "NAV Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      1.00      0.65       597\n",
      "        1.0       0.71      0.01      0.02       653\n",
      "\n",
      "avg / total       0.60      0.48      0.32      1250\n",
      "\n",
      "[[595   2]\n",
      " [648   5]]\n",
      "NMC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.83      0.62       600\n",
      "        1.0       0.57      0.20      0.30       650\n",
      "\n",
      "avg / total       0.53      0.51      0.45      1250\n",
      "\n",
      "[[500 100]\n",
      " [518 132]]\n",
      "NXT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.00      0.01       612\n",
      "        1.0       0.51      1.00      0.68       638\n",
      "\n",
      "avg / total       0.51      0.51      0.35      1250\n",
      "\n",
      "[[  2 610]\n",
      " [  2 636]]\n",
      "DCR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.04      0.08       625\n",
      "        1.0       0.50      0.96      0.66       625\n",
      "\n",
      "avg / total       0.50      0.50      0.37      1250\n",
      "\n",
      "[[ 28 597]\n",
      " [ 28 597]]\n",
      "FLO Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.98      0.65       612\n",
      "        1.0       0.54      0.02      0.04       638\n",
      "\n",
      "avg / total       0.51      0.49      0.34      1250\n",
      "\n",
      "[[600  12]\n",
      " [624  14]]\n",
      "REP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.01      0.03       613\n",
      "        1.0       0.51      0.99      0.67       637\n",
      "\n",
      "avg / total       0.52      0.51      0.36      1250\n",
      "\n",
      "[[  9 604]\n",
      " [  8 629]]\n",
      "BCY Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.93      0.65       620\n",
      "        1.0       0.54      0.08      0.14       630\n",
      "\n",
      "avg / total       0.52      0.50      0.39      1250\n",
      "\n",
      "[[576  44]\n",
      " [578  52]]\n",
      "FCT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.99      0.66       622\n",
      "        1.0       0.64      0.02      0.04       628\n",
      "\n",
      "avg / total       0.57      0.50      0.35      1250\n",
      "\n",
      "[[614   8]\n",
      " [614  14]]\n",
      "XRP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      1.00      0.65       608\n",
      "        1.0       0.00      0.00      0.00       642\n",
      "\n",
      "avg / total       0.24      0.49      0.32      1250\n",
      "\n",
      "[[608   0]\n",
      " [642   0]]\n",
      "CLAM Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.84      0.63       621\n",
      "        1.0       0.54      0.19      0.28       624\n",
      "\n",
      "avg / total       0.52      0.51      0.45      1245\n",
      "\n",
      "[[521 100]\n",
      " [508 116]]\n",
      "STRAT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       630\n",
      "        1.0       0.50      1.00      0.66       620\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1250\n",
      "\n",
      "[[  0 630]\n",
      " [  0 620]]\n",
      "NAUT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.98      0.66       621\n",
      "        1.0       0.55      0.02      0.04       629\n",
      "\n",
      "avg / total       0.52      0.50      0.35      1250\n",
      "\n",
      "[[611  10]\n",
      " [617  12]]\n",
      "MAID Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      1.00      0.68       642\n",
      "        1.0       0.25      0.00      0.00       608\n",
      "\n",
      "avg / total       0.38      0.51      0.35      1250\n",
      "\n",
      "[[639   3]\n",
      " [607   1]]\n",
      "PPC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.84      0.61       594\n",
      "        1.0       0.52      0.16      0.24       656\n",
      "\n",
      "avg / total       0.50      0.48      0.41      1250\n",
      "\n",
      "[[499  95]\n",
      " [553 103]]\n",
      "VIA Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.94      0.63       601\n",
      "        1.0       0.44      0.04      0.08       649\n",
      "\n",
      "avg / total       0.46      0.47      0.34      1250\n",
      "\n",
      "[[565  36]\n",
      " [621  28]]\n",
      "XCP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.71      0.58       615\n",
      "        1.0       0.51      0.29      0.37       635\n",
      "\n",
      "avg / total       0.50      0.50      0.48      1250\n",
      "\n",
      "[[438 177]\n",
      " [450 185]]\n",
      "XPM Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.42      0.10      0.17       610\n",
      "        1.0       0.50      0.86      0.63       640\n",
      "\n",
      "avg / total       0.46      0.49      0.41      1250\n",
      "\n",
      "[[ 63 547]\n",
      " [ 88 552]]\n",
      "ETH Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       635\n",
      "        1.0       0.49      1.00      0.66       615\n",
      "\n",
      "avg / total       0.24      0.49      0.32      1250\n",
      "\n",
      "[[  0 635]\n",
      " [  0 615]]\n",
      "DASH Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      0.04      0.07       647\n",
      "        1.0       0.49      0.98      0.65       603\n",
      "\n",
      "avg / total       0.55      0.49      0.35      1250\n",
      "\n",
      "[[ 24 623]\n",
      " [ 15 588]]\n",
      "BTS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       1.00      0.00      0.00       647\n",
      "        1.0       0.48      1.00      0.65       603\n",
      "\n",
      "avg / total       0.75      0.48      0.32      1250\n",
      "\n",
      "[[  1 646]\n",
      " [  0 603]]\n",
      "SYS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      1.00      0.66       618\n",
      "        1.0       0.67      0.00      0.01       632\n",
      "\n",
      "avg / total       0.58      0.50      0.33      1250\n",
      "\n",
      "[[617   1]\n",
      " [630   2]]\n",
      "NOTE Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.94      0.67       644\n",
      "        1.0       0.45      0.05      0.09       606\n",
      "\n",
      "avg / total       0.48      0.51      0.39      1250\n",
      "\n",
      "[[608  36]\n",
      " [576  30]]\n",
      "FLDC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      1.00      0.71       692\n",
      "        1.0       0.00      0.00      0.00       558\n",
      "\n",
      "avg / total       0.31      0.55      0.39      1250\n",
      "\n",
      "[[692   0]\n",
      " [558   0]]\n",
      "EMC2 Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.59      1.00      0.74       741\n",
      "        1.0       0.00      0.00      0.00       509\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1250\n",
      "\n",
      "[[738   3]\n",
      " [509   0]]\n",
      "SC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      1.00      0.75       746\n",
      "        1.0       0.00      0.00      0.00       499\n",
      "\n",
      "avg / total       0.36      0.60      0.45      1245\n",
      "\n",
      "[[746   0]\n",
      " [499   0]]\n",
      "PINK Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      1.00      0.71       683\n",
      "        1.0       0.00      0.00      0.00       567\n",
      "\n",
      "avg / total       0.30      0.55      0.39      1250\n",
      "\n",
      "[[683   0]\n",
      " [567   0]]\n",
      "XMR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       616\n",
      "        1.0       0.51      1.00      0.67       634\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1250\n",
      "\n",
      "[[  0 616]\n",
      " [  0 634]]\n",
      "HUC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.95      0.63       595\n",
      "        1.0       0.39      0.03      0.05       655\n",
      "\n",
      "avg / total       0.43      0.47      0.33      1250\n",
      "\n",
      "[[567  28]\n",
      " [637  18]]\n",
      "SJCX Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.92      0.64       603\n",
      "        1.0       0.56      0.10      0.17       647\n",
      "\n",
      "avg / total       0.52      0.49      0.39      1250\n",
      "\n",
      "[[553  50]\n",
      " [584  63]]\n",
      "XBC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.98      0.66       625\n",
      "        1.0       0.50      0.02      0.03       625\n",
      "\n",
      "avg / total       0.50      0.50      0.35      1250\n",
      "\n",
      "[[615  10]\n",
      " [615  10]]\n",
      "STR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      1.00      0.68       650\n",
      "        1.0       0.00      0.00      0.00       600\n",
      "\n",
      "avg / total       0.27      0.52      0.36      1250\n",
      "\n",
      "[[650   0]\n",
      " [600   0]]\n",
      "DGB Logistic Regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.63      1.00      0.77       782\n",
      "        1.0       0.00      0.00      0.00       468\n",
      "\n",
      "avg / total       0.39      0.63      0.48      1250\n",
      "\n",
      "[[782   0]\n",
      " [468   0]]\n",
      "BLK Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.95      0.66       632\n",
      "        1.0       0.46      0.05      0.09       618\n",
      "\n",
      "avg / total       0.48      0.50      0.37      1250\n",
      "\n",
      "[[598  34]\n",
      " [589  29]]\n",
      "ZEC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       558\n",
      "        1.0       0.55      1.00      0.71       692\n",
      "\n",
      "avg / total       0.31      0.55      0.39      1250\n",
      "\n",
      "[[  0 558]\n",
      " [  1 691]]\n",
      "NXC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       617\n",
      "        1.0       0.51      1.00      0.67       633\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1250\n",
      "\n",
      "[[  0 617]\n",
      " [  0 633]]\n",
      "POT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      1.00      0.66       622\n",
      "        1.0       1.00      0.00      0.00       628\n",
      "\n",
      "avg / total       0.75      0.50      0.33      1250\n",
      "\n",
      "[[622   0]\n",
      " [627   1]]\n",
      "LSK Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       609\n",
      "        1.0       0.51      1.00      0.68       641\n",
      "\n",
      "avg / total       0.26      0.51      0.35      1250\n",
      "\n",
      "[[  0 609]\n",
      " [  1 640]]\n",
      "GAME Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.99      0.65       604\n",
      "        1.0       0.68      0.03      0.06       646\n",
      "\n",
      "avg / total       0.59      0.49      0.34      1250\n",
      "\n",
      "[[595   9]\n",
      " [627  19]]\n",
      "BTCD Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.83      0.63       650\n",
      "        1.0       0.43      0.14      0.21       600\n",
      "\n",
      "avg / total       0.47      0.50      0.43      1250\n",
      "\n",
      "[[541 109]\n",
      " [519  81]]\n",
      "VRC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.91      0.63       612\n",
      "        1.0       0.49      0.09      0.15       638\n",
      "\n",
      "avg / total       0.49      0.49      0.39      1250\n",
      "\n",
      "[[554  58]\n",
      " [582  56]]\n",
      "LBC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       609\n",
      "        1.0       0.51      1.00      0.68       641\n",
      "\n",
      "avg / total       0.26      0.51      0.35      1250\n",
      "\n",
      "[[  0 609]\n",
      " [  1 640]]\n",
      "NEOS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.91      0.63       606\n",
      "        1.0       0.53      0.10      0.17       644\n",
      "\n",
      "avg / total       0.51      0.49      0.39      1250\n",
      "\n",
      "[[550  56]\n",
      " [581  63]]\n",
      "LTC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.33      0.00      0.00       572\n",
      "        1.0       0.54      1.00      0.70       673\n",
      "\n",
      "avg / total       0.45      0.54      0.38      1245\n",
      "\n",
      "[[  1 571]\n",
      " [  2 671]]\n",
      "VTC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.05      0.09       617\n",
      "        1.0       0.51      0.95      0.66       633\n",
      "\n",
      "avg / total       0.49      0.50      0.38      1250\n",
      "\n",
      "[[ 29 588]\n",
      " [ 33 600]]\n",
      "OMNI Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.90      0.65       632\n",
      "        1.0       0.46      0.09      0.14       618\n",
      "\n",
      "avg / total       0.48      0.50      0.40      1250\n",
      "\n",
      "[[571  61]\n",
      " [565  53]]\n",
      "GRC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.98      0.67       636\n",
      "        1.0       0.66      0.03      0.06       614\n",
      "\n",
      "avg / total       0.58      0.52      0.37      1250\n",
      "\n",
      "[[626  10]\n",
      " [595  19]]\n",
      "EXP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.07      0.12       611\n",
      "        1.0       0.51      0.92      0.66       639\n",
      "\n",
      "avg / total       0.49      0.51      0.40      1250\n",
      "\n",
      "[[ 43 568]\n",
      " [ 49 590]]\n",
      "XEM Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      1.00      0.69       659\n",
      "        1.0       0.00      0.00      0.00       591\n",
      "\n",
      "avg / total       0.28      0.53      0.36      1250\n",
      "\n",
      "[[659   0]\n",
      " [591   0]]\n",
      "RADS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.21      0.29       598\n",
      "        1.0       0.52      0.80      0.63       652\n",
      "\n",
      "avg / total       0.51      0.52      0.47      1250\n",
      "\n",
      "[[126 472]\n",
      " [133 519]]\n",
      "ETC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       591\n",
      "        1.0       0.53      1.00      0.69       659\n",
      "\n",
      "avg / total       0.28      0.53      0.36      1250\n",
      "\n",
      "[[  0 591]\n",
      " [  0 659]]\n",
      "ARDR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       630\n",
      "        1.0       0.50      1.00      0.66       620\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1250\n",
      "\n",
      "[[  0 630]\n",
      " [  0 620]]\n",
      "BELA Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      1.00      0.67       630\n",
      "        1.0       1.00      0.00      0.00       620\n",
      "\n",
      "avg / total       0.75      0.50      0.34      1250\n",
      "\n",
      "[[630   0]\n",
      " [619   1]]\n",
      "AMP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.00      0.01       611\n",
      "        1.0       0.51      1.00      0.68       639\n",
      "\n",
      "avg / total       0.51      0.51      0.35      1250\n",
      "\n",
      "[[  2 609]\n",
      " [  2 637]]\n"
     ]
    }
   ],
   "source": [
    "for coin in colarr:\n",
    "    crypt = df[coin]\n",
    "    trainX = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00'].drop('up',axis=1)\n",
    "    trainY = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00']['up']\n",
    "    testX = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()].drop('up',axis=1)\n",
    "    testY = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()]['up']\n",
    "    \n",
    "    print coin + ' SVC'\n",
    "    \n",
    "    model = SVC()\n",
    "    model.fit(trainX,trainY)\n",
    "    pred = model.predict(testX)\n",
    "    print classification_report(testY,pred)\n",
    "    print confusion_matrix(testY,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOGE SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.66      0.67       834\n",
      "        1.0       0.37      0.39      0.38       416\n",
      "\n",
      "avg / total       0.58      0.57      0.57      1250\n",
      "\n",
      "[[551 283]\n",
      " [253 163]]\n",
      "NAV SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.39      0.43       597\n",
      "        1.0       0.52      0.60      0.56       653\n",
      "\n",
      "avg / total       0.49      0.50      0.49      1250\n",
      "\n",
      "[[232 365]\n",
      " [262 391]]\n",
      "NMC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.32      0.38       600\n",
      "        1.0       0.52      0.69      0.60       650\n",
      "\n",
      "avg / total       0.51      0.51      0.49      1250\n",
      "\n",
      "[[189 411]\n",
      " [199 451]]\n",
      "NXT SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.47      0.47       612\n",
      "        1.0       0.49      0.49      0.49       638\n",
      "\n",
      "avg / total       0.48      0.48      0.48      1250\n",
      "\n",
      "[[287 325]\n",
      " [323 315]]\n",
      "DCR SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.71      0.59       625\n",
      "        1.0       0.50      0.30      0.37       625\n",
      "\n",
      "avg / total       0.50      0.50      0.48      1250\n",
      "\n",
      "[[443 182]\n",
      " [440 185]]\n",
      "FLO SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.29      0.37       612\n",
      "        1.0       0.52      0.72      0.60       638\n",
      "\n",
      "avg / total       0.51      0.51      0.49      1250\n",
      "\n",
      "[[180 432]\n",
      " [176 462]]\n",
      "REP SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.23      0.32       613\n",
      "        1.0       0.52      0.78      0.62       637\n",
      "\n",
      "avg / total       0.51      0.51      0.47      1250\n",
      "\n",
      "[[144 469]\n",
      " [138 499]]\n",
      "BCY SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.46      0.47       620\n",
      "        1.0       0.49      0.51      0.50       630\n",
      "\n",
      "avg / total       0.49      0.49      0.49      1250\n",
      "\n",
      "[[286 334]\n",
      " [307 323]]\n",
      "FCT SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.57      0.52       622\n",
      "        1.0       0.48      0.40      0.44       628\n",
      "\n",
      "avg / total       0.48      0.48      0.48      1250\n",
      "\n",
      "[[353 269]\n",
      " [375 253]]\n",
      "XRP SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.29      0.37       608\n",
      "        1.0       0.53      0.75      0.62       642\n",
      "\n",
      "avg / total       0.52      0.53      0.50      1250\n",
      "\n",
      "[[174 434]\n",
      " [159 483]]\n",
      "CLAM SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.60      0.55       621\n",
      "        1.0       0.50      0.41      0.45       624\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1245\n",
      "\n",
      "[[372 249]\n",
      " [370 254]]\n",
      "STRAT SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.14      0.22       630\n",
      "        1.0       0.50      0.87      0.63       620\n",
      "\n",
      "avg / total       0.51      0.50      0.43      1250\n",
      "\n",
      "[[ 87 543]\n",
      " [ 79 541]]\n",
      "NAUT SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.56      0.53       621\n",
      "        1.0       0.50      0.44      0.47       629\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1250\n",
      "\n",
      "[[348 273]\n",
      " [351 278]]\n",
      "MAID SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.48      0.50       642\n",
      "        1.0       0.49      0.52      0.50       608\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1250\n",
      "\n",
      "[[309 333]\n",
      " [292 316]]\n",
      "PPC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.32      0.39       594\n",
      "        1.0       0.53      0.70      0.61       656\n",
      "\n",
      "avg / total       0.52      0.52      0.50      1250\n",
      "\n",
      "[[193 401]\n",
      " [196 460]]\n",
      "VIA SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.27      0.34       601\n",
      "        1.0       0.52      0.73      0.61       649\n",
      "\n",
      "avg / total       0.50      0.51      0.48      1250\n",
      "\n",
      "[[161 440]\n",
      " [176 473]]\n",
      "XCP SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.48      0.48       615\n",
      "        1.0       0.50      0.51      0.50       635\n",
      "\n",
      "avg / total       0.49      0.49      0.49      1250\n",
      "\n",
      "[[293 322]\n",
      " [313 322]]\n",
      "XPM SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.46      0.46      0.46       610\n",
      "        1.0       0.49      0.50      0.49       640\n",
      "\n",
      "avg / total       0.48      0.48      0.48      1250\n",
      "\n",
      "[[278 332]\n",
      " [323 317]]\n",
      "ETH SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.56      0.25      0.34       635\n",
      "        1.0       0.51      0.80      0.62       615\n",
      "\n",
      "avg / total       0.53      0.52      0.48      1250\n",
      "\n",
      "[[156 479]\n",
      " [124 491]]\n",
      "DASH SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.15      0.24       647\n",
      "        1.0       0.49      0.87      0.62       603\n",
      "\n",
      "avg / total       0.52      0.50      0.42      1250\n",
      "\n",
      "[[ 97 550]\n",
      " [ 79 524]]\n",
      "BTS SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.62      0.57       647\n",
      "        1.0       0.50      0.41      0.45       603\n",
      "\n",
      "avg / total       0.52      0.52      0.51      1250\n",
      "\n",
      "[[400 247]\n",
      " [356 247]]\n",
      "SYS SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.29      0.37       618\n",
      "        1.0       0.52      0.76      0.62       632\n",
      "\n",
      "avg / total       0.53      0.53      0.50      1250\n",
      "\n",
      "[[177 441]\n",
      " [150 482]]\n",
      "NOTE SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.52      0.51       644\n",
      "        1.0       0.48      0.47      0.48       606\n",
      "\n",
      "avg / total       0.49      0.50      0.50      1250\n",
      "\n",
      "[[333 311]\n",
      " [320 286]]\n",
      "FLDC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.33      0.43       692\n",
      "        1.0       0.47      0.73      0.57       558\n",
      "\n",
      "avg / total       0.54      0.51      0.49      1250\n",
      "\n",
      "[[229 463]\n",
      " [151 407]]\n",
      "EMC2 SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.48      0.56       741\n",
      "        1.0       0.48      0.69      0.56       509\n",
      "\n",
      "avg / total       0.60      0.56      0.56      1250\n",
      "\n",
      "[[353 388]\n",
      " [157 352]]\n",
      "SC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.65      0.47      0.55       746\n",
      "        1.0       0.44      0.61      0.51       499\n",
      "\n",
      "avg / total       0.56      0.53      0.53      1245\n",
      "\n",
      "[[354 392]\n",
      " [194 305]]\n",
      "PINK SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.59      0.45      0.51       683\n",
      "        1.0       0.48      0.62      0.54       567\n",
      "\n",
      "avg / total       0.54      0.53      0.52      1250\n",
      "\n",
      "[[304 379]\n",
      " [213 354]]\n",
      "XMR SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.37      0.42       616\n",
      "        1.0       0.50      0.61      0.55       634\n",
      "\n",
      "avg / total       0.49      0.49      0.48      1250\n",
      "\n",
      "[[226 390]\n",
      " [247 387]]\n",
      "HUC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.50      0.50       595\n",
      "        1.0       0.55      0.56      0.56       655\n",
      "\n",
      "avg / total       0.53      0.53      0.53      1250\n",
      "\n",
      "[[298 297]\n",
      " [288 367]]\n",
      "SJCX SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.48      0.49       603\n",
      "        1.0       0.54      0.56      0.55       647\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1250\n",
      "\n",
      "[[288 315]\n",
      " [284 363]]\n",
      "XBC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      0.23      0.34       625\n",
      "        1.0       0.53      0.86      0.65       625\n",
      "\n",
      "avg / total       0.57      0.54      0.49      1250\n",
      "\n",
      "[[144 481]\n",
      " [ 89 536]]\n",
      "STR SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.49      0.52       650\n",
      "        1.0       0.50      0.56      0.53       600\n",
      "\n",
      "avg / total       0.53      0.52      0.52      1250\n",
      "\n",
      "[[318 332]\n",
      " [263 337]]\n",
      "DGB SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.65      0.59      0.62       782\n",
      "        1.0       0.40      0.46      0.43       468\n",
      "\n",
      "avg / total       0.55      0.54      0.55      1250\n",
      "\n",
      "[[462 320]\n",
      " [253 215]]\n",
      "BLK SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.32      0.38       632\n",
      "        1.0       0.48      0.65      0.56       618\n",
      "\n",
      "avg / total       0.48      0.48      0.47      1250\n",
      "\n",
      "[[201 431]\n",
      " [214 404]]\n",
      "ZEC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.43      0.50      0.46       558\n",
      "        1.0       0.53      0.46      0.49       692\n",
      "\n",
      "avg / total       0.49      0.48      0.48      1250\n",
      "\n",
      "[[280 278]\n",
      " [373 319]]\n",
      "NXC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.04      0.07       617\n",
      "        1.0       0.51      0.96      0.66       633\n",
      "\n",
      "avg / total       0.50      0.51      0.37      1250\n",
      "\n",
      "[[ 23 594]\n",
      " [ 24 609]]\n",
      "POT SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.67      0.58       622\n",
      "        1.0       0.52      0.36      0.43       628\n",
      "\n",
      "avg / total       0.51      0.51      0.50      1250\n",
      "\n",
      "[[414 208]\n",
      " [402 226]]\n",
      "LSK SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.45      0.47       609\n",
      "        1.0       0.51      0.55      0.53       641\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1250\n",
      "\n",
      "[[276 333]\n",
      " [289 352]]\n",
      "GAME SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.45      0.23      0.30       604\n",
      "        1.0       0.50      0.74      0.60       646\n",
      "\n",
      "avg / total       0.48      0.49      0.45      1250\n",
      "\n",
      "[[137 467]\n",
      " [170 476]]\n",
      "BTCD SVC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.53      0.52       650\n",
      "        1.0       0.46      0.43      0.44       600\n",
      "\n",
      "avg / total       0.48      0.48      0.48      1250\n",
      "\n",
      "[[345 305]\n",
      " [344 256]]\n",
      "VRC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.50      0.48       612\n",
      "        1.0       0.50      0.47      0.48       638\n",
      "\n",
      "avg / total       0.49      0.48      0.48      1250\n",
      "\n",
      "[[303 309]\n",
      " [335 303]]\n",
      "LBC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.44      0.47       609\n",
      "        1.0       0.53      0.61      0.57       641\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1250\n",
      "\n",
      "[[268 341]\n",
      " [253 388]]\n",
      "NEOS SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.53      0.51       606\n",
      "        1.0       0.52      0.48      0.50       644\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1250\n",
      "\n",
      "[[323 283]\n",
      " [338 306]]\n",
      "LTC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.46      0.67      0.55       572\n",
      "        1.0       0.54      0.32      0.40       673\n",
      "\n",
      "avg / total       0.50      0.49      0.47      1245\n",
      "\n",
      "[[386 186]\n",
      " [455 218]]\n",
      "VTC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.43      0.46       617\n",
      "        1.0       0.52      0.59      0.55       633\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1250\n",
      "\n",
      "[[264 353]\n",
      " [257 376]]\n",
      "OMNI SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.59      0.55       632\n",
      "        1.0       0.50      0.41      0.45       618\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1250\n",
      "\n",
      "[[375 257]\n",
      " [364 254]]\n",
      "GRC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.49      0.49       636\n",
      "        1.0       0.48      0.50      0.49       614\n",
      "\n",
      "avg / total       0.49      0.49      0.49      1250\n",
      "\n",
      "[[309 327]\n",
      " [307 307]]\n",
      "EXP SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.45      0.47       611\n",
      "        1.0       0.51      0.55      0.53       639\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1250\n",
      "\n",
      "[[273 338]\n",
      " [290 349]]\n",
      "XEM SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.29      0.38       659\n",
      "        1.0       0.48      0.74      0.59       591\n",
      "\n",
      "avg / total       0.52      0.50      0.48      1250\n",
      "\n",
      "[[189 470]\n",
      " [152 439]]\n",
      "RADS SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.41      0.45       598\n",
      "        1.0       0.54      0.64      0.58       652\n",
      "\n",
      "avg / total       0.53      0.53      0.52      1250\n",
      "\n",
      "[[245 353]\n",
      " [237 415]]\n",
      "ETC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.42      0.45       591\n",
      "        1.0       0.54      0.60      0.57       659\n",
      "\n",
      "avg / total       0.51      0.52      0.51      1250\n",
      "\n",
      "[[248 343]\n",
      " [263 396]]\n",
      "ARDR SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.20      0.29       630\n",
      "        1.0       0.50      0.83      0.62       620\n",
      "\n",
      "avg / total       0.52      0.51      0.45      1250\n",
      "\n",
      "[[123 507]\n",
      " [108 512]]\n",
      "BELA SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.56      0.10      0.16       630\n",
      "        1.0       0.50      0.92      0.65       620\n",
      "\n",
      "avg / total       0.53      0.51      0.40      1250\n",
      "\n",
      "[[ 60 570]\n",
      " [ 47 573]]\n",
      "AMP SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.47      0.48       611\n",
      "        1.0       0.51      0.54      0.52       639\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1250\n",
      "\n",
      "[[286 325]\n",
      " [296 343]]\n"
     ]
    }
   ],
   "source": [
    "for coin in colarr:\n",
    "    crypt = df[coin]\n",
    "    trainX = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00'].drop('up',axis=1)\n",
    "    trainY = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00']['up']\n",
    "    testX = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()].drop('up',axis=1)\n",
    "    testY = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()]['up']\n",
    "    \n",
    "    print coin + ' SVC'\n",
    "    \n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(trainX,trainY)\n",
    "    pred = model.predict(testX)\n",
    "    print classification_report(testY,pred)\n",
    "    print confusion_matrix(testY,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
