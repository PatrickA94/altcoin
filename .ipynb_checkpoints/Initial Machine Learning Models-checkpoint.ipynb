{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge to single Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "### Check for missing data\n",
    "### Write function that evaluates each model for each currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/home/patrick/Documents/Alt Coin Proj/data/newfeat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.drop(['BTC','USDT','PASC','GNT','GNO'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "colarr = []\n",
    "for col in df.columns.unique():\n",
    "    colarr.append(col[0])\n",
    "colarr = pd.Series(colarr)\n",
    "colarr = colarr.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOGE Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.71      0.64      0.67       834\n",
      "        1.0       0.40      0.47      0.43       416\n",
      "\n",
      "avg / total       0.60      0.58      0.59      1250\n",
      "\n",
      "[[537 297]\n",
      " [222 194]]\n",
      "NAV Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.08      0.13       597\n",
      "        1.0       0.53      0.94      0.67       653\n",
      "\n",
      "avg / total       0.53      0.53      0.42      1250\n",
      "\n",
      "[[ 46 551]\n",
      " [ 41 612]]\n",
      "NMC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.80      0.60       600\n",
      "        1.0       0.50      0.18      0.26       650\n",
      "\n",
      "avg / total       0.49      0.48      0.42      1250\n",
      "\n",
      "[[482 118]\n",
      " [533 117]]\n",
      "NXT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.14      0.22       612\n",
      "        1.0       0.52      0.88      0.65       638\n",
      "\n",
      "avg / total       0.53      0.52      0.44      1250\n",
      "\n",
      "[[ 87 525]\n",
      " [ 75 563]]\n",
      "DCR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.66      0.56       625\n",
      "        1.0       0.48      0.32      0.38       625\n",
      "\n",
      "avg / total       0.49      0.49      0.47      1250\n",
      "\n",
      "[[413 212]\n",
      " [427 198]]\n",
      "FLO Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.02      0.05       612\n",
      "        1.0       0.51      0.98      0.67       638\n",
      "\n",
      "avg / total       0.51      0.51      0.37      1250\n",
      "\n",
      "[[ 15 597]\n",
      " [ 15 623]]\n",
      "REP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.14      0.22       613\n",
      "        1.0       0.51      0.87      0.65       637\n",
      "\n",
      "avg / total       0.51      0.51      0.44      1250\n",
      "\n",
      "[[ 85 528]\n",
      " [ 82 555]]\n",
      "BCY Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.49      0.49       620\n",
      "        1.0       0.49      0.48      0.48       630\n",
      "\n",
      "avg / total       0.48      0.48      0.48      1250\n",
      "\n",
      "[[306 314]\n",
      " [330 300]]\n",
      "FCT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.41      0.46       622\n",
      "        1.0       0.53      0.65      0.58       628\n",
      "\n",
      "avg / total       0.53      0.53      0.52      1250\n",
      "\n",
      "[[252 370]\n",
      " [218 410]]\n",
      "XRP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.80      0.59       608\n",
      "        1.0       0.45      0.16      0.23       642\n",
      "\n",
      "avg / total       0.46      0.47      0.41      1250\n",
      "\n",
      "[[484 124]\n",
      " [542 100]]\n",
      "CLAM Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.57      0.33      0.42       621\n",
      "        1.0       0.53      0.75      0.62       624\n",
      "\n",
      "avg / total       0.55      0.54      0.52      1245\n",
      "\n",
      "[[206 415]\n",
      " [158 466]]\n",
      "STRAT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.38      0.44       630\n",
      "        1.0       0.50      0.63      0.56       620\n",
      "\n",
      "avg / total       0.51      0.51      0.50      1250\n",
      "\n",
      "[[241 389]\n",
      " [229 391]]\n",
      "NAUT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.89      0.64       621\n",
      "        1.0       0.53      0.13      0.21       629\n",
      "\n",
      "avg / total       0.52      0.50      0.42      1250\n",
      "\n",
      "[[550  71]\n",
      " [548  81]]\n",
      "MAID Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.56      0.40      0.46       642\n",
      "        1.0       0.51      0.67      0.58       608\n",
      "\n",
      "avg / total       0.53      0.53      0.52      1250\n",
      "\n",
      "[[255 387]\n",
      " [203 405]]\n",
      "PPC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.72      0.57       594\n",
      "        1.0       0.53      0.29      0.37       656\n",
      "\n",
      "avg / total       0.51      0.49      0.47      1250\n",
      "\n",
      "[[427 167]\n",
      " [467 189]]\n",
      "VIA Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.50      0.49       601\n",
      "        1.0       0.52      0.51      0.51       649\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1250\n",
      "\n",
      "[[299 302]\n",
      " [320 329]]\n",
      "XCP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.57      0.09      0.15       615\n",
      "        1.0       0.51      0.94      0.66       635\n",
      "\n",
      "avg / total       0.54      0.52      0.41      1250\n",
      "\n",
      "[[ 54 561]\n",
      " [ 40 595]]\n",
      "XPM Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.69      0.58       610\n",
      "        1.0       0.54      0.35      0.42       640\n",
      "\n",
      "avg / total       0.52      0.51      0.50      1250\n",
      "\n",
      "[[422 188]\n",
      " [419 221]]\n",
      "ETH Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.12      0.20       635\n",
      "        1.0       0.50      0.92      0.65       615\n",
      "\n",
      "avg / total       0.55      0.51      0.42      1250\n",
      "\n",
      "[[ 77 558]\n",
      " [ 51 564]]\n",
      "DASH Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.70      0.59       647\n",
      "        1.0       0.45      0.27      0.34       603\n",
      "\n",
      "avg / total       0.48      0.49      0.46      1250\n",
      "\n",
      "[[451 196]\n",
      " [442 161]]\n",
      "BTS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.16      0.25       647\n",
      "        1.0       0.50      0.89      0.64       603\n",
      "\n",
      "avg / total       0.55      0.51      0.44      1250\n",
      "\n",
      "[[102 545]\n",
      " [ 68 535]]\n",
      "SYS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.67      0.57       618\n",
      "        1.0       0.52      0.36      0.42       632\n",
      "\n",
      "avg / total       0.51      0.51      0.50      1250\n",
      "\n",
      "[[411 207]\n",
      " [407 225]]\n",
      "NOTE Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.52      0.54       644\n",
      "        1.0       0.52      0.54      0.53       606\n",
      "\n",
      "avg / total       0.53      0.53      0.53      1250\n",
      "\n",
      "[[338 306]\n",
      " [281 325]]\n",
      "FLDC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.66      0.30      0.41       692\n",
      "        1.0       0.48      0.81      0.61       558\n",
      "\n",
      "avg / total       0.58      0.53      0.50      1250\n",
      "\n",
      "[[206 486]\n",
      " [105 453]]\n",
      "EMC2 Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.61      0.20      0.30       741\n",
      "        1.0       0.41      0.81      0.55       509\n",
      "\n",
      "avg / total       0.53      0.45      0.40      1250\n",
      "\n",
      "[[146 595]\n",
      " [ 95 414]]\n",
      "SC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      1.00      0.75       746\n",
      "        1.0       0.00      0.00      0.00       499\n",
      "\n",
      "avg / total       0.36      0.60      0.45      1245\n",
      "\n",
      "[[746   0]\n",
      " [499   0]]\n",
      "PINK Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.68      0.19      0.30       683\n",
      "        1.0       0.48      0.89      0.62       567\n",
      "\n",
      "avg / total       0.59      0.51      0.45      1250\n",
      "\n",
      "[[132 551]\n",
      " [ 63 504]]\n",
      "XMR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.56      0.44      0.49       616\n",
      "        1.0       0.55      0.67      0.60       634\n",
      "\n",
      "avg / total       0.55      0.55      0.55      1250\n",
      "\n",
      "[[270 346]\n",
      " [212 422]]\n",
      "HUC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.02      0.03       595\n",
      "        1.0       0.53      0.99      0.69       655\n",
      "\n",
      "avg / total       0.56      0.53      0.37      1250\n",
      "\n",
      "[[  9 586]\n",
      " [  6 649]]\n",
      "SJCX Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.58      0.05      0.10       603\n",
      "        1.0       0.52      0.96      0.68       647\n",
      "\n",
      "avg / total       0.55      0.52      0.40      1250\n",
      "\n",
      "[[ 33 570]\n",
      " [ 24 623]]\n",
      "XBC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.58      0.16      0.25       625\n",
      "        1.0       0.51      0.88      0.65       625\n",
      "\n",
      "avg / total       0.55      0.52      0.45      1250\n",
      "\n",
      "[[100 525]\n",
      " [ 72 553]]\n",
      "STR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.57      0.39      0.47       650\n",
      "        1.0       0.51      0.68      0.58       600\n",
      "\n",
      "avg / total       0.54      0.53      0.52      1250\n",
      "\n",
      "[[256 394]\n",
      " [195 405]]\n",
      "DGB Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      1.00      0.77       782\n",
      "        1.0       0.00      0.00      0.00       468\n",
      "\n",
      "avg / total       0.39      0.62      0.48      1250\n",
      "\n",
      "[[779   3]\n",
      " [468   0]]\n",
      "BLK Logistic Regression\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.58      0.55       632\n",
      "        1.0       0.52      0.48      0.50       618\n",
      "\n",
      "avg / total       0.53      0.53      0.53      1250\n",
      "\n",
      "[[365 267]\n",
      " [324 294]]\n",
      "ZEC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.41      0.17      0.24       558\n",
      "        1.0       0.55      0.81      0.65       692\n",
      "\n",
      "avg / total       0.49      0.52      0.47      1250\n",
      "\n",
      "[[ 93 465]\n",
      " [132 560]]\n",
      "NXC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.73      0.03      0.05       617\n",
      "        1.0       0.51      0.99      0.67       633\n",
      "\n",
      "avg / total       0.62      0.51      0.37      1250\n",
      "\n",
      "[[ 16 601]\n",
      " [  6 627]]\n",
      "POT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.92      0.64       622\n",
      "        1.0       0.47      0.07      0.11       628\n",
      "\n",
      "avg / total       0.48      0.49      0.38      1250\n",
      "\n",
      "[[575  47]\n",
      " [587  41]]\n",
      "LSK Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.24      0.33       609\n",
      "        1.0       0.52      0.79      0.63       641\n",
      "\n",
      "avg / total       0.52      0.52      0.48      1250\n",
      "\n",
      "[[145 464]\n",
      " [134 507]]\n",
      "GAME Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.15      0.24       604\n",
      "        1.0       0.52      0.86      0.65       646\n",
      "\n",
      "avg / total       0.52      0.52      0.45      1250\n",
      "\n",
      "[[ 93 511]\n",
      " [ 88 558]]\n",
      "BTCD Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.97      0.68       650\n",
      "        1.0       0.61      0.05      0.09       600\n",
      "\n",
      "avg / total       0.57      0.53      0.40      1250\n",
      "\n",
      "[[631  19]\n",
      " [570  30]]\n",
      "VRC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.47      0.50       612\n",
      "        1.0       0.54      0.59      0.57       638\n",
      "\n",
      "avg / total       0.53      0.54      0.53      1250\n",
      "\n",
      "[[290 322]\n",
      " [259 379]]\n",
      "LBC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.27      0.35       609\n",
      "        1.0       0.52      0.74      0.61       641\n",
      "\n",
      "avg / total       0.51      0.51      0.48      1250\n",
      "\n",
      "[[162 447]\n",
      " [165 476]]\n",
      "NEOS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.06      0.11       606\n",
      "        1.0       0.51      0.93      0.66       644\n",
      "\n",
      "avg / total       0.50      0.51      0.40      1250\n",
      "\n",
      "[[ 38 568]\n",
      " [ 42 602]]\n",
      "LTC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.68      0.56       572\n",
      "        1.0       0.57      0.36      0.44       673\n",
      "\n",
      "avg / total       0.53      0.51      0.50      1245\n",
      "\n",
      "[[391 181]\n",
      " [432 241]]\n",
      "VTC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.00      0.01       617\n",
      "        1.0       0.51      1.00      0.67       633\n",
      "\n",
      "avg / total       0.55      0.51      0.35      1250\n",
      "\n",
      "[[  3 614]\n",
      " [  2 631]]\n",
      "OMNI Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.66      0.57       632\n",
      "        1.0       0.50      0.35      0.41       618\n",
      "\n",
      "avg / total       0.50      0.50      0.49      1250\n",
      "\n",
      "[[414 218]\n",
      " [403 215]]\n",
      "GRC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.58      0.20      0.30       636\n",
      "        1.0       0.51      0.85      0.63       614\n",
      "\n",
      "avg / total       0.54      0.52      0.46      1250\n",
      "\n",
      "[[127 509]\n",
      " [ 93 521]]\n",
      "EXP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      0.04      0.08       611\n",
      "        1.0       0.52      0.98      0.67       639\n",
      "\n",
      "avg / total       0.57      0.52      0.38      1250\n",
      "\n",
      "[[ 25 586]\n",
      " [ 15 624]]\n",
      "XEM Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.32      0.41       659\n",
      "        1.0       0.48      0.71      0.58       591\n",
      "\n",
      "avg / total       0.52      0.51      0.49      1250\n",
      "\n",
      "[[213 446]\n",
      " [172 419]]\n",
      "RADS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.58      0.53       598\n",
      "        1.0       0.54      0.46      0.49       652\n",
      "\n",
      "avg / total       0.52      0.52      0.51      1250\n",
      "\n",
      "[[347 251]\n",
      " [355 297]]\n",
      "ETC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.46      0.22      0.30       591\n",
      "        1.0       0.52      0.76      0.62       659\n",
      "\n",
      "avg / total       0.49      0.51      0.47      1250\n",
      "\n",
      "[[130 461]\n",
      " [155 504]]\n",
      "ARDR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.21      0.30       630\n",
      "        1.0       0.50      0.80      0.62       620\n",
      "\n",
      "avg / total       0.51      0.51      0.46      1250\n",
      "\n",
      "[[135 495]\n",
      " [123 497]]\n",
      "BELA Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.64      0.01      0.02       630\n",
      "        1.0       0.50      0.99      0.66       620\n",
      "\n",
      "avg / total       0.57      0.50      0.34      1250\n",
      "\n",
      "[[  7 623]\n",
      " [  4 616]]\n",
      "AMP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.33      0.41       611\n",
      "        1.0       0.54      0.74      0.62       639\n",
      "\n",
      "avg / total       0.54      0.54      0.52      1250\n",
      "\n",
      "[[202 409]\n",
      " [166 473]]\n"
     ]
    }
   ],
   "source": [
    "for coin in colarr:\n",
    "    crypt = df[coin]\n",
    "    trainX = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00'].drop('up',axis=1)\n",
    "    trainY = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00']['up']\n",
    "    testX = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()].drop('up',axis=1)\n",
    "    testY = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()]['up']\n",
    "    \n",
    "    print coin + ' Logistic Regression'\n",
    "    \n",
    "    logmodel = LogisticRegression()\n",
    "    logmodel.fit(trainX,trainY)\n",
    "    pred = logmodel.predict(testX)\n",
    "    print classification_report(testY,pred)\n",
    "    print confusion_matrix(testY,pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOGE SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.67      1.00      0.80       834\n",
      "        1.0       0.00      0.00      0.00       416\n",
      "\n",
      "avg / total       0.45      0.67      0.53      1250\n",
      "\n",
      "[[834   0]\n",
      " [416   0]]\n",
      "NAV SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      1.00      0.65       597\n",
      "        1.0       0.00      0.00      0.00       653\n",
      "\n",
      "avg / total       0.23      0.48      0.31      1250\n",
      "\n",
      "[[597   0]\n",
      " [653   0]]\n",
      "NMC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      1.00      0.65       600\n",
      "        1.0       0.00      0.00      0.00       650\n",
      "\n",
      "avg / total       0.23      0.48      0.31      1250\n",
      "\n",
      "[[600   0]\n",
      " [650   0]]\n",
      "NXT SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       612\n",
      "        1.0       0.51      1.00      0.68       638\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1250\n",
      "\n",
      "[[  0 612]\n",
      " [  0 638]]\n",
      "DCR SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       625\n",
      "        1.0       0.50      1.00      0.67       625\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1250\n",
      "\n",
      "[[  0 625]\n",
      " [  0 625]]\n",
      "FLO SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      1.00      0.66       612\n",
      "        1.0       0.00      0.00      0.00       638\n",
      "\n",
      "avg / total       0.24      0.49      0.32      1250\n",
      "\n",
      "[[612   0]\n",
      " [638   0]]\n",
      "REP SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       613\n",
      "        1.0       0.51      1.00      0.68       637\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1250\n",
      "\n",
      "[[  0 613]\n",
      " [  0 637]]\n",
      "BCY SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      1.00      0.66       620\n",
      "        1.0       0.00      0.00      0.00       630\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1250\n",
      "\n",
      "[[620   0]\n",
      " [630   0]]\n",
      "FCT SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      1.00      0.66       622\n",
      "        1.0       0.00      0.00      0.00       628\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1250\n",
      "\n",
      "[[622   0]\n",
      " [628   0]]\n",
      "XRP SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      1.00      0.65       608\n",
      "        1.0       0.00      0.00      0.00       642\n",
      "\n",
      "avg / total       0.24      0.49      0.32      1250\n",
      "\n",
      "[[608   0]\n",
      " [642   0]]\n",
      "CLAM SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      1.00      0.67       621\n",
      "        1.0       0.00      0.00      0.00       624\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1245\n",
      "\n",
      "[[621   0]\n",
      " [624   0]]\n",
      "STRAT SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      1.00      0.67       630\n",
      "        1.0       0.00      0.00      0.00       620\n",
      "\n",
      "avg / total       0.25      0.50      0.34      1250\n",
      "\n",
      "[[630   0]\n",
      " [620   0]]\n",
      "NAUT SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      1.00      0.66       621\n",
      "        1.0       0.00      0.00      0.00       629\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1250\n",
      "\n",
      "[[621   0]\n",
      " [629   0]]\n",
      "MAID SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      1.00      0.68       642\n",
      "        1.0       0.00      0.00      0.00       608\n",
      "\n",
      "avg / total       0.26      0.51      0.35      1250\n",
      "\n",
      "[[642   0]\n",
      " [608   0]]\n",
      "PPC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      1.00      0.64       594\n",
      "        1.0       0.00      0.00      0.00       656\n",
      "\n",
      "avg / total       0.23      0.48      0.31      1250\n",
      "\n",
      "[[594   0]\n",
      " [656   0]]\n",
      "VIA SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      1.00      0.65       601\n",
      "        1.0       0.00      0.00      0.00       649\n",
      "\n",
      "avg / total       0.23      0.48      0.31      1250\n",
      "\n",
      "[[601   0]\n",
      " [649   0]]\n",
      "XCP SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      1.00      0.66       615\n",
      "        1.0       0.00      0.00      0.00       635\n",
      "\n",
      "avg / total       0.24      0.49      0.32      1250\n",
      "\n",
      "[[615   0]\n",
      " [635   0]]\n",
      "XPM SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      1.00      0.66       610\n",
      "        1.0       0.00      0.00      0.00       640\n",
      "\n",
      "avg / total       0.24      0.49      0.32      1250\n",
      "\n",
      "[[610   0]\n",
      " [640   0]]\n",
      "ETH SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       635\n",
      "        1.0       0.49      1.00      0.66       615\n",
      "\n",
      "avg / total       0.24      0.49      0.32      1250\n",
      "\n",
      "[[  0 635]\n",
      " [  0 615]]\n",
      "DASH SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       647\n",
      "        1.0       0.48      1.00      0.65       603\n",
      "\n",
      "avg / total       0.23      0.48      0.31      1250\n",
      "\n",
      "[[  0 647]\n",
      " [  0 603]]\n",
      "BTS SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      1.00      0.68       647\n",
      "        1.0       0.00      0.00      0.00       603\n",
      "\n",
      "avg / total       0.27      0.52      0.35      1250\n",
      "\n",
      "[[647   0]\n",
      " [603   0]]\n",
      "SYS SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      1.00      0.66       618\n",
      "        1.0       0.00      0.00      0.00       632\n",
      "\n",
      "avg / total       0.24      0.49      0.33      1250\n",
      "\n",
      "[[618   0]\n",
      " [632   0]]\n",
      "NOTE SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      1.00      0.68       644\n",
      "        1.0       0.00      0.00      0.00       606\n",
      "\n",
      "avg / total       0.27      0.52      0.35      1250\n",
      "\n",
      "[[644   0]\n",
      " [606   0]]\n",
      "FLDC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      1.00      0.71       692\n",
      "        1.0       0.00      0.00      0.00       558\n",
      "\n",
      "avg / total       0.31      0.55      0.39      1250\n",
      "\n",
      "[[692   0]\n",
      " [558   0]]\n",
      "EMC2 SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.59      1.00      0.74       741\n",
      "        1.0       0.00      0.00      0.00       509\n",
      "\n",
      "avg / total       0.35      0.59      0.44      1250\n",
      "\n",
      "[[741   0]\n",
      " [509   0]]\n",
      "SC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      1.00      0.75       746\n",
      "        1.0       0.00      0.00      0.00       499\n",
      "\n",
      "avg / total       0.36      0.60      0.45      1245\n",
      "\n",
      "[[746   0]\n",
      " [499   0]]\n",
      "PINK SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      1.00      0.71       683\n",
      "        1.0       0.00      0.00      0.00       567\n",
      "\n",
      "avg / total       0.30      0.55      0.39      1250\n",
      "\n",
      "[[683   0]\n",
      " [567   0]]\n",
      "XMR SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      1.00      0.66       616\n",
      "        1.0       0.00      0.00      0.00       634\n",
      "\n",
      "avg / total       0.24      0.49      0.33      1250\n",
      "\n",
      "[[616   0]\n",
      " [634   0]]\n",
      "HUC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      1.00      0.64       595\n",
      "        1.0       0.00      0.00      0.00       655\n",
      "\n",
      "avg / total       0.23      0.48      0.31      1250\n",
      "\n",
      "[[595   0]\n",
      " [655   0]]\n",
      "SJCX SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      1.00      0.65       603\n",
      "        1.0       0.00      0.00      0.00       647\n",
      "\n",
      "avg / total       0.23      0.48      0.31      1250\n",
      "\n",
      "[[603   0]\n",
      " [647   0]]\n",
      "XBC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      1.00      0.67       625\n",
      "        1.0       0.00      0.00      0.00       625\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1250\n",
      "\n",
      "[[625   0]\n",
      " [625   0]]\n",
      "STR SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      1.00      0.68       650\n",
      "        1.0       0.00      0.00      0.00       600\n",
      "\n",
      "avg / total       0.27      0.52      0.36      1250\n",
      "\n",
      "[[650   0]\n",
      " [600   0]]\n",
      "DGB SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.63      1.00      0.77       782\n",
      "        1.0       0.00      0.00      0.00       468\n",
      "\n",
      "avg / total       0.39      0.63      0.48      1250\n",
      "\n",
      "[[782   0]\n",
      " [468   0]]\n",
      "BLK SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      1.00      0.67       632\n",
      "        1.0       0.00      0.00      0.00       618\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1250\n",
      "\n",
      "[[632   0]\n",
      " [618   0]]\n",
      "ZEC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       558\n",
      "        1.0       0.55      1.00      0.71       692\n",
      "\n",
      "avg / total       0.31      0.55      0.39      1250\n",
      "\n",
      "[[  0 558]\n",
      " [  0 692]]\n",
      "NXC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       617\n",
      "        1.0       0.51      1.00      0.67       633\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1250\n",
      "\n",
      "[[  0 617]\n",
      " [  0 633]]\n",
      "POT SVC\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      1.00      0.66       622\n",
      "        1.0       0.00      0.00      0.00       628\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1250\n",
      "\n",
      "[[622   0]\n",
      " [628   0]]\n",
      "LSK SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       609\n",
      "        1.0       0.51      1.00      0.68       641\n",
      "\n",
      "avg / total       0.26      0.51      0.35      1250\n",
      "\n",
      "[[  0 609]\n",
      " [  0 641]]\n",
      "GAME SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      1.00      0.65       604\n",
      "        1.0       0.00      0.00      0.00       646\n",
      "\n",
      "avg / total       0.23      0.48      0.31      1250\n",
      "\n",
      "[[604   0]\n",
      " [646   0]]\n",
      "BTCD SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      1.00      0.68       650\n",
      "        1.0       0.00      0.00      0.00       600\n",
      "\n",
      "avg / total       0.27      0.52      0.36      1250\n",
      "\n",
      "[[650   0]\n",
      " [600   0]]\n",
      "VRC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      1.00      0.66       612\n",
      "        1.0       0.00      0.00      0.00       638\n",
      "\n",
      "avg / total       0.24      0.49      0.32      1250\n",
      "\n",
      "[[612   0]\n",
      " [638   0]]\n",
      "LBC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       609\n",
      "        1.0       0.51      1.00      0.68       641\n",
      "\n",
      "avg / total       0.26      0.51      0.35      1250\n",
      "\n",
      "[[  0 609]\n",
      " [  0 641]]\n",
      "NEOS SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      1.00      0.65       606\n",
      "        1.0       0.00      0.00      0.00       644\n",
      "\n",
      "avg / total       0.24      0.48      0.32      1250\n",
      "\n",
      "[[606   0]\n",
      " [644   0]]\n",
      "LTC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       572\n",
      "        1.0       0.54      1.00      0.70       673\n",
      "\n",
      "avg / total       0.29      0.54      0.38      1245\n",
      "\n",
      "[[  0 572]\n",
      " [  0 673]]\n",
      "VTC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      1.00      0.66       617\n",
      "        1.0       0.00      0.00      0.00       633\n",
      "\n",
      "avg / total       0.24      0.49      0.33      1250\n",
      "\n",
      "[[617   0]\n",
      " [633   0]]\n",
      "OMNI SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      1.00      0.67       632\n",
      "        1.0       0.00      0.00      0.00       618\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1250\n",
      "\n",
      "[[632   0]\n",
      " [618   0]]\n",
      "GRC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      1.00      0.67       636\n",
      "        1.0       0.00      0.00      0.00       614\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1250\n",
      "\n",
      "[[636   0]\n",
      " [614   0]]\n",
      "EXP SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       611\n",
      "        1.0       0.51      1.00      0.68       639\n",
      "\n",
      "avg / total       0.26      0.51      0.35      1250\n",
      "\n",
      "[[  0 611]\n",
      " [  0 639]]\n",
      "XEM SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      1.00      0.69       659\n",
      "        1.0       0.00      0.00      0.00       591\n",
      "\n",
      "avg / total       0.28      0.53      0.36      1250\n",
      "\n",
      "[[659   0]\n",
      " [591   0]]\n",
      "RADS SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       598\n",
      "        1.0       0.52      1.00      0.69       652\n",
      "\n",
      "avg / total       0.27      0.52      0.36      1250\n",
      "\n",
      "[[  0 598]\n",
      " [  0 652]]\n",
      "ETC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       591\n",
      "        1.0       0.53      1.00      0.69       659\n",
      "\n",
      "avg / total       0.28      0.53      0.36      1250\n",
      "\n",
      "[[  0 591]\n",
      " [  0 659]]\n",
      "ARDR SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       630\n",
      "        1.0       0.50      1.00      0.66       620\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1250\n",
      "\n",
      "[[  0 630]\n",
      " [  0 620]]\n",
      "BELA SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      1.00      0.67       630\n",
      "        1.0       0.00      0.00      0.00       620\n",
      "\n",
      "avg / total       0.25      0.50      0.34      1250\n",
      "\n",
      "[[630   0]\n",
      " [620   0]]\n",
      "AMP SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       611\n",
      "        1.0       0.51      1.00      0.68       639\n",
      "\n",
      "avg / total       0.26      0.51      0.35      1250\n",
      "\n",
      "[[  0 611]\n",
      " [  0 639]]\n"
     ]
    }
   ],
   "source": [
    "for coin in colarr:\n",
    "    crypt = df[coin]\n",
    "    trainX = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00'].drop('up',axis=1)\n",
    "    trainY = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00']['up']\n",
    "    testX = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()].drop('up',axis=1)\n",
    "    testY = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()]['up']\n",
    "    \n",
    "    print coin + ' SVC'\n",
    "    \n",
    "    model = SVC()\n",
    "    model.fit(trainX,trainY)\n",
    "    pred = model.predict(testX)\n",
    "    print classification_report(testY,pred)\n",
    "    print confusion_matrix(testY,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOGE TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.68      0.56      0.61       834\n",
      "        1.0       0.35      0.47      0.40       416\n",
      "\n",
      "avg / total       0.57      0.53      0.54      1250\n",
      "\n",
      "[[464 370]\n",
      " [219 197]]\n",
      "NAV TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.44      0.46       597\n",
      "        1.0       0.53      0.57      0.55       653\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1250\n",
      "\n",
      "[[264 333]\n",
      " [282 371]]\n",
      "NMC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.37      0.41       600\n",
      "        1.0       0.51      0.62      0.56       650\n",
      "\n",
      "avg / total       0.49      0.50      0.49      1250\n",
      "\n",
      "[[220 380]\n",
      " [249 401]]\n",
      "NXT TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.43      0.47       612\n",
      "        1.0       0.52      0.59      0.55       638\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1250\n",
      "\n",
      "[[265 347]\n",
      " [261 377]]\n",
      "DCR TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.24      0.33       625\n",
      "        1.0       0.50      0.77      0.61       625\n",
      "\n",
      "avg / total       0.50      0.50      0.47      1250\n",
      "\n",
      "[[150 475]\n",
      " [146 479]]\n",
      "FLO TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.36      0.43       612\n",
      "        1.0       0.53      0.68      0.59       638\n",
      "\n",
      "avg / total       0.52      0.53      0.51      1250\n",
      "\n",
      "[[222 390]\n",
      " [203 435]]\n",
      "REP TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.54      0.52       613\n",
      "        1.0       0.52      0.48      0.50       637\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1250\n",
      "\n",
      "[[330 283]\n",
      " [331 306]]\n",
      "BCY TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.34      0.41       620\n",
      "        1.0       0.52      0.70      0.59       630\n",
      "\n",
      "avg / total       0.52      0.52      0.50      1250\n",
      "\n",
      "[[209 411]\n",
      " [192 438]]\n",
      "FCT TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.51      0.51       622\n",
      "        1.0       0.51      0.51      0.51       628\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1250\n",
      "\n",
      "[[318 304]\n",
      " [307 321]]\n",
      "XRP TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.49      0.49       608\n",
      "        1.0       0.53      0.54      0.53       642\n",
      "\n",
      "avg / total       0.51      0.52      0.51      1250\n",
      "\n",
      "[[297 311]\n",
      " [295 347]]\n",
      "CLAM TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.49      0.49       621\n",
      "        1.0       0.50      0.52      0.51       624\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1245\n",
      "\n",
      "[[302 319]\n",
      " [302 322]]\n",
      "STRAT TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.58      0.56       630\n",
      "        1.0       0.53      0.49      0.51       620\n",
      "\n",
      "avg / total       0.54      0.54      0.53      1250\n",
      "\n",
      "[[365 265]\n",
      " [316 304]]\n",
      "NAUT TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.61      0.54       621\n",
      "        1.0       0.49      0.38      0.43       629\n",
      "\n",
      "avg / total       0.49      0.49      0.48      1250\n",
      "\n",
      "[[378 243]\n",
      " [393 236]]\n",
      "MAID TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.53      0.52       642\n",
      "        1.0       0.49      0.48      0.48       608\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1250\n",
      "\n",
      "[[342 300]\n",
      " [319 289]]\n",
      "PPC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.46      0.27      0.34       594\n",
      "        1.0       0.52      0.72      0.60       656\n",
      "\n",
      "avg / total       0.49      0.50      0.48      1250\n",
      "\n",
      "[[161 433]\n",
      " [186 470]]\n",
      "VIA TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.46      0.57      0.51       601\n",
      "        1.0       0.50      0.39      0.44       649\n",
      "\n",
      "avg / total       0.48      0.48      0.47      1250\n",
      "\n",
      "[[341 260]\n",
      " [393 256]]\n",
      "XCP TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.44      0.46       615\n",
      "        1.0       0.50      0.54      0.52       635\n",
      "\n",
      "avg / total       0.49      0.49      0.49      1250\n",
      "\n",
      "[[272 343]\n",
      " [291 344]]\n",
      "XPM TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.33      0.40       610\n",
      "        1.0       0.52      0.69      0.59       640\n",
      "\n",
      "avg / total       0.51      0.51      0.50      1250\n",
      "\n",
      "[[204 406]\n",
      " [201 439]]\n",
      "ETH TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.47      0.50       635\n",
      "        1.0       0.51      0.57      0.54       615\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1250\n",
      "\n",
      "[[298 337]\n",
      " [267 348]]\n",
      "DASH TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.56      0.24      0.34       647\n",
      "        1.0       0.49      0.79      0.61       603\n",
      "\n",
      "avg / total       0.53      0.51      0.47      1250\n",
      "\n",
      "[[157 490]\n",
      " [124 479]]\n",
      "BTS TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.39      0.44       647\n",
      "        1.0       0.47      0.58      0.52       603\n",
      "\n",
      "avg / total       0.49      0.48      0.48      1250\n",
      "\n",
      "[[255 392]\n",
      " [253 350]]\n",
      "SYS TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.60      0.55       618\n",
      "        1.0       0.52      0.41      0.46       632\n",
      "\n",
      "avg / total       0.51      0.51      0.50      1250\n",
      "\n",
      "[[372 246]\n",
      " [370 262]]\n",
      "NOTE TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.49      0.50       644\n",
      "        1.0       0.49      0.52      0.51       606\n",
      "\n",
      "avg / total       0.51      0.50      0.50      1250\n",
      "\n",
      "[[313 331]\n",
      " [288 318]]\n",
      "FLDC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.65      0.31      0.42       692\n",
      "        1.0       0.48      0.79      0.60       558\n",
      "\n",
      "avg / total       0.57      0.53      0.50      1250\n",
      "\n",
      "[[217 475]\n",
      " [117 441]]\n",
      "EMC2 TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      0.57      0.60       741\n",
      "        1.0       0.44      0.48      0.46       509\n",
      "\n",
      "avg / total       0.55      0.54      0.54      1250\n",
      "\n",
      "[[426 315]\n",
      " [263 246]]\n",
      "SC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.59      0.39      0.47       746\n",
      "        1.0       0.40      0.60      0.48       499\n",
      "\n",
      "avg / total       0.51      0.47      0.47      1245\n",
      "\n",
      "[[290 456]\n",
      " [199 300]]\n",
      "PINK TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.48      0.51       683\n",
      "        1.0       0.44      0.50      0.47       567\n",
      "\n",
      "avg / total       0.50      0.49      0.49      1250\n",
      "\n",
      "[[330 353]\n",
      " [284 283]]\n",
      "XMR TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.44      0.47       616\n",
      "        1.0       0.52      0.59      0.55       634\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1250\n",
      "\n",
      "[[268 348]\n",
      " [259 375]]\n",
      "HUC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.45      0.52      0.49       595\n",
      "        1.0       0.50      0.43      0.46       655\n",
      "\n",
      "avg / total       0.48      0.47      0.47      1250\n",
      "\n",
      "[[312 283]\n",
      " [374 281]]\n",
      "SJCX TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.45      0.47       603\n",
      "        1.0       0.52      0.56      0.54       647\n",
      "\n",
      "avg / total       0.50      0.51      0.50      1250\n",
      "\n",
      "[[270 333]\n",
      " [285 362]]\n",
      "XBC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.32      0.39       625\n",
      "        1.0       0.50      0.67      0.57       625\n",
      "\n",
      "avg / total       0.50      0.50      0.48      1250\n",
      "\n",
      "[[201 424]\n",
      " [205 420]]\n",
      "STR TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.41      0.47       650\n",
      "        1.0       0.49      0.62      0.55       600\n",
      "\n",
      "avg / total       0.52      0.51      0.51      1250\n",
      "\n",
      "[[267 383]\n",
      " [227 373]]\n",
      "DGB TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.67      0.58      0.62       782\n",
      "        1.0       0.43      0.52      0.47       468\n",
      "\n",
      "avg / total       0.58      0.56      0.57      1250\n",
      "\n",
      "[[456 326]\n",
      " [225 243]]\n",
      "BLK TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.47      0.49       632\n",
      "        1.0       0.50      0.55      0.52       618\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1250\n",
      "\n",
      "[[298 334]\n",
      " [281 337]]\n",
      "ZEC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.41      0.44       558\n",
      "        1.0       0.57      0.63      0.60       692\n",
      "\n",
      "avg / total       0.53      0.53      0.53      1250\n",
      "\n",
      "[[227 331]\n",
      " [255 437]]\n",
      "NXC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.56      0.16      0.25       617\n",
      "        1.0       0.52      0.88      0.65       633\n",
      "\n",
      "avg / total       0.54      0.52      0.45      1250\n",
      "\n",
      "[[100 517]\n",
      " [ 79 554]]\n",
      "POT TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.40      0.46       622\n",
      "        1.0       0.52      0.64      0.57       628\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1250\n",
      "\n",
      "[[251 371]\n",
      " [225 403]]\n",
      "LSK TREE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.45      0.48       609\n",
      "        1.0       0.53      0.58      0.55       641\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1250\n",
      "\n",
      "[[276 333]\n",
      " [270 371]]\n",
      "GAME TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.53      0.51       604\n",
      "        1.0       0.52      0.48      0.50       646\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1250\n",
      "\n",
      "[[318 286]\n",
      " [335 311]]\n",
      "BTCD TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.42      0.46       650\n",
      "        1.0       0.48      0.58      0.52       600\n",
      "\n",
      "avg / total       0.50      0.50      0.49      1250\n",
      "\n",
      "[[272 378]\n",
      " [252 348]]\n",
      "VRC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.47      0.47       612\n",
      "        1.0       0.50      0.50      0.50       638\n",
      "\n",
      "avg / total       0.49      0.49      0.49      1250\n",
      "\n",
      "[[288 324]\n",
      " [317 321]]\n",
      "LBC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.54      0.52       609\n",
      "        1.0       0.53      0.50      0.51       641\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1250\n",
      "\n",
      "[[328 281]\n",
      " [323 318]]\n",
      "NEOS TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.46      0.49       606\n",
      "        1.0       0.54      0.61      0.57       644\n",
      "\n",
      "avg / total       0.53      0.53      0.53      1250\n",
      "\n",
      "[[276 330]\n",
      " [252 392]]\n",
      "LTC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.46      0.35      0.40       572\n",
      "        1.0       0.54      0.65      0.59       673\n",
      "\n",
      "avg / total       0.50      0.51      0.50      1245\n",
      "\n",
      "[[202 370]\n",
      " [238 435]]\n",
      "VTC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.34      0.40       617\n",
      "        1.0       0.51      0.67      0.58       633\n",
      "\n",
      "avg / total       0.50      0.51      0.49      1250\n",
      "\n",
      "[[209 408]\n",
      " [210 423]]\n",
      "OMNI TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.59      0.55       632\n",
      "        1.0       0.51      0.43      0.46       618\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1250\n",
      "\n",
      "[[375 257]\n",
      " [355 263]]\n",
      "GRC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.47      0.50       636\n",
      "        1.0       0.52      0.59      0.55       614\n",
      "\n",
      "avg / total       0.53      0.53      0.53      1250\n",
      "\n",
      "[[297 339]\n",
      " [252 362]]\n",
      "EXP TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.39      0.46       611\n",
      "        1.0       0.54      0.69      0.61       639\n",
      "\n",
      "avg / total       0.54      0.54      0.53      1250\n",
      "\n",
      "[[239 372]\n",
      " [198 441]]\n",
      "XEM TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.47      0.50       659\n",
      "        1.0       0.48      0.54      0.51       591\n",
      "\n",
      "avg / total       0.51      0.50      0.50      1250\n",
      "\n",
      "[[309 350]\n",
      " [271 320]]\n",
      "RADS TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.53      0.52       598\n",
      "        1.0       0.56      0.55      0.55       652\n",
      "\n",
      "avg / total       0.54      0.54      0.54      1250\n",
      "\n",
      "[[314 284]\n",
      " [293 359]]\n",
      "ETC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.46      0.47       591\n",
      "        1.0       0.52      0.53      0.53       659\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1250\n",
      "\n",
      "[[274 317]\n",
      " [309 350]]\n",
      "ARDR TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.25      0.33       630\n",
      "        1.0       0.49      0.74      0.59       620\n",
      "\n",
      "avg / total       0.49      0.49      0.46      1250\n",
      "\n",
      "[[159 471]\n",
      " [162 458]]\n",
      "BELA TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.26      0.35       630\n",
      "        1.0       0.50      0.75      0.60       620\n",
      "\n",
      "avg / total       0.51      0.50      0.47      1250\n",
      "\n",
      "[[165 465]\n",
      " [154 466]]\n",
      "AMP TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.45      0.47       611\n",
      "        1.0       0.52      0.59      0.55       639\n",
      "\n",
      "avg / total       0.52      0.52      0.51      1250\n",
      "\n",
      "[[272 339]\n",
      " [265 374]]\n"
     ]
    }
   ],
   "source": [
    "for coin in colarr:\n",
    "    crypt = df[coin]\n",
    "    trainX = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00'].drop('up',axis=1)\n",
    "    trainY = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00']['up']\n",
    "    testX = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()].drop('up',axis=1)\n",
    "    testY = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()]['up']\n",
    "    \n",
    "    print coin + ' TREE'\n",
    "    \n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(trainX,trainY)\n",
    "    pred = model.predict(testX)\n",
    "    print classification_report(testY,pred)\n",
    "    print confusion_matrix(testY,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
