{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge to single Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "### Check for missing data\n",
    "### Write function that evaluates each model for each currency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('/home/patrick/Documents/Alt Coin Proj/data/newfeat.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = df.drop(['BTC','USDT','PASC','GNT','GNO'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colarr = []\n",
    "for col in df.columns.unique():\n",
    "    colarr.append(col[0])\n",
    "colarr = pd.Series(colarr)\n",
    "colarr = colarr.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOGE Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.71      0.61      0.65       882\n",
      "        1.0       0.40      0.51      0.44       447\n",
      "\n",
      "avg / total       0.60      0.57      0.58      1329\n",
      "\n",
      "[[536 346]\n",
      " [221 226]]\n",
      "NAV Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.09      0.15       638\n",
      "        1.0       0.53      0.93      0.67       691\n",
      "\n",
      "avg / total       0.54      0.53      0.42      1329\n",
      "\n",
      "[[ 56 582]\n",
      " [ 45 646]]\n",
      "NMC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.77      0.58       634\n",
      "        1.0       0.49      0.20      0.29       695\n",
      "\n",
      "avg / total       0.48      0.47      0.43      1329\n",
      "\n",
      "[[489 145]\n",
      " [555 140]]\n",
      "NXT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.28      0.36       649\n",
      "        1.0       0.51      0.73      0.60       680\n",
      "\n",
      "avg / total       0.51      0.51      0.48      1329\n",
      "\n",
      "[[180 469]\n",
      " [183 497]]\n",
      "DCR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.66      0.56       625\n",
      "        1.0       0.48      0.32      0.38       625\n",
      "\n",
      "avg / total       0.49      0.49      0.47      1250\n",
      "\n",
      "[[413 212]\n",
      " [427 198]]\n",
      "FLO Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.02      0.04       650\n",
      "        1.0       0.51      0.98      0.67       679\n",
      "\n",
      "avg / total       0.51      0.51      0.36      1329\n",
      "\n",
      "[[ 15 635]\n",
      " [ 15 664]]\n",
      "REP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.14      0.22       654\n",
      "        1.0       0.51      0.87      0.64       675\n",
      "\n",
      "avg / total       0.51      0.51      0.43      1329\n",
      "\n",
      "[[ 90 564]\n",
      " [ 86 589]]\n",
      "BCY Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.52      0.50       662\n",
      "        1.0       0.49      0.46      0.47       667\n",
      "\n",
      "avg / total       0.49      0.49      0.49      1329\n",
      "\n",
      "[[343 319]\n",
      " [360 307]]\n",
      "FCT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.40      0.46       665\n",
      "        1.0       0.52      0.66      0.58       664\n",
      "\n",
      "avg / total       0.53      0.53      0.52      1329\n",
      "\n",
      "[[264 401]\n",
      " [228 436]]\n",
      "XRP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.81      0.59       641\n",
      "        1.0       0.45      0.15      0.22       688\n",
      "\n",
      "avg / total       0.46      0.46      0.40      1329\n",
      "\n",
      "[[517 124]\n",
      " [588 100]]\n",
      "CLAM Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.57      0.31      0.40       663\n",
      "        1.0       0.53      0.76      0.62       666\n",
      "\n",
      "avg / total       0.55      0.54      0.51      1329\n",
      "\n",
      "[[208 455]\n",
      " [157 509]]\n",
      "STRAT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.36      0.42       668\n",
      "        1.0       0.50      0.65      0.57       661\n",
      "\n",
      "avg / total       0.51      0.51      0.50      1329\n",
      "\n",
      "[[241 427]\n",
      " [229 432]]\n",
      "NAUT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.91      0.65       659\n",
      "        1.0       0.54      0.10      0.17       670\n",
      "\n",
      "avg / total       0.52      0.50      0.41      1329\n",
      "\n",
      "[[601  58]\n",
      " [602  68]]\n",
      "MAID Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.38      0.45       680\n",
      "        1.0       0.51      0.68      0.58       649\n",
      "\n",
      "avg / total       0.53      0.52      0.51      1329\n",
      "\n",
      "[[257 423]\n",
      " [209 440]]\n",
      "PPC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.73      0.57       632\n",
      "        1.0       0.52      0.26      0.35       697\n",
      "\n",
      "avg / total       0.49      0.48      0.46      1329\n",
      "\n",
      "[[459 173]\n",
      " [513 184]]\n",
      "VIA Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.50      0.50       640\n",
      "        1.0       0.53      0.51      0.52       689\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1329\n",
      "\n",
      "[[323 317]\n",
      " [338 351]]\n",
      "XCP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.44      0.04      0.08       647\n",
      "        1.0       0.51      0.95      0.66       682\n",
      "\n",
      "avg / total       0.48      0.51      0.38      1329\n",
      "\n",
      "[[ 28 619]\n",
      " [ 35 647]]\n",
      "XPM Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.72      0.59       648\n",
      "        1.0       0.54      0.31      0.40       681\n",
      "\n",
      "avg / total       0.52      0.51      0.49      1329\n",
      "\n",
      "[[468 180]\n",
      " [469 212]]\n",
      "ETH Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.61      0.12      0.20       670\n",
      "        1.0       0.51      0.92      0.65       659\n",
      "\n",
      "avg / total       0.56      0.52      0.42      1329\n",
      "\n",
      "[[ 79 591]\n",
      " [ 51 608]]\n",
      "DASH Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.72      0.59       683\n",
      "        1.0       0.46      0.25      0.32       646\n",
      "\n",
      "avg / total       0.48      0.49      0.46      1329\n",
      "\n",
      "[[493 190]\n",
      " [486 160]]\n",
      "RIC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.08      0.14       636\n",
      "        1.0       0.53      0.94      0.67       693\n",
      "\n",
      "avg / total       0.53      0.53      0.42      1329\n",
      "\n",
      "[[ 50 586]\n",
      " [ 43 650]]\n",
      "BTS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.15      0.24       684\n",
      "        1.0       0.50      0.89      0.64       645\n",
      "\n",
      "avg / total       0.55      0.51      0.43      1329\n",
      "\n",
      "[[102 582]\n",
      " [ 68 577]]\n",
      "SYS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.69      0.58       659\n",
      "        1.0       0.52      0.34      0.41       670\n",
      "\n",
      "avg / total       0.51      0.51      0.49      1329\n",
      "\n",
      "[[452 207]\n",
      " [445 225]]\n",
      "NOTE Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.46      0.50       685\n",
      "        1.0       0.51      0.59      0.55       644\n",
      "\n",
      "avg / total       0.53      0.53      0.53      1329\n",
      "\n",
      "[[318 367]\n",
      " [261 383]]\n",
      "FLDC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.66      0.28      0.40       726\n",
      "        1.0       0.49      0.83      0.61       603\n",
      "\n",
      "avg / total       0.58      0.53      0.50      1329\n",
      "\n",
      "[[206 520]\n",
      " [105 498]]\n",
      "EMC2 Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.19      0.29       776\n",
      "        1.0       0.42      0.82      0.56       553\n",
      "\n",
      "avg / total       0.53      0.45      0.40      1329\n",
      "\n",
      "[[147 629]\n",
      " [ 97 456]]\n",
      "SC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.59      1.00      0.74       780\n",
      "        1.0       0.00      0.00      0.00       549\n",
      "\n",
      "avg / total       0.34      0.59      0.43      1329\n",
      "\n",
      "[[780   0]\n",
      " [549   0]]\n",
      "PINK Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.68      0.16      0.26       716\n",
      "        1.0       0.48      0.91      0.63       613\n",
      "\n",
      "avg / total       0.59      0.51      0.43      1329\n",
      "\n",
      "[[117 599]\n",
      " [ 54 559]]\n",
      "XMR Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.56      0.42      0.48       653\n",
      "        1.0       0.55      0.68      0.60       676\n",
      "\n",
      "avg / total       0.55      0.55      0.54      1329\n",
      "\n",
      "[[275 378]\n",
      " [219 457]]\n",
      "HUC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.03      0.05       634\n",
      "        1.0       0.52      0.98      0.68       695\n",
      "\n",
      "avg / total       0.54      0.53      0.38      1329\n",
      "\n",
      "[[ 17 617]\n",
      " [ 14 681]]\n",
      "SJCX Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.58      0.05      0.09       646\n",
      "        1.0       0.52      0.96      0.67       683\n",
      "\n",
      "avg / total       0.55      0.52      0.39      1329\n",
      "\n",
      "[[ 33 613]\n",
      " [ 24 659]]\n",
      "XBC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.57      0.41      0.48       657\n",
      "        1.0       0.55      0.70      0.61       672\n",
      "\n",
      "avg / total       0.56      0.56      0.55      1329\n",
      "\n",
      "[[268 389]\n",
      " [201 471]]\n",
      "STR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.57      0.37      0.45       691\n",
      "        1.0       0.50      0.69      0.58       638\n",
      "\n",
      "avg / total       0.54      0.53      0.51      1329\n",
      "\n",
      "[[256 435]\n",
      " [195 443]]\n",
      "DGB Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.61      1.00      0.76       818\n",
      "        1.0       0.00      0.00      0.00       511\n",
      "\n",
      "avg / total       0.38      0.61      0.47      1329\n",
      "\n",
      "[[815   3]\n",
      " [511   0]]\n",
      "BLK Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.53      0.53       666\n",
      "        1.0       0.52      0.52      0.52       663\n",
      "\n",
      "avg / total       0.53      0.53      0.53      1329\n",
      "\n",
      "[[352 314]\n",
      " [317 346]]\n",
      "ZEC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.41      0.16      0.23       594\n",
      "        1.0       0.55      0.82      0.66       735\n",
      "\n",
      "avg / total       0.49      0.52      0.46      1329\n",
      "\n",
      "[[ 93 501]\n",
      " [132 603]]\n",
      "NXC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.73      0.02      0.05       654\n",
      "        1.0       0.51      0.99      0.68       675\n",
      "\n",
      "avg / total       0.62      0.52      0.37      1329\n",
      "\n",
      "[[ 16 638]\n",
      " [  6 669]]\n",
      "POT Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.93      0.65       663\n",
      "        1.0       0.46      0.06      0.11       666\n",
      "\n",
      "avg / total       0.48      0.49      0.38      1329\n",
      "\n",
      "[[616  47]\n",
      " [626  40]]\n",
      "LSK Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.22      0.31       645\n",
      "        1.0       0.52      0.80      0.63       684\n",
      "\n",
      "avg / total       0.52      0.52      0.48      1329\n",
      "\n",
      "[[145 500]\n",
      " [134 550]]\n",
      "GAME Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.14      0.23       645\n",
      "        1.0       0.52      0.87      0.65       684\n",
      "\n",
      "avg / total       0.52      0.52      0.44      1329\n",
      "\n",
      "[[ 93 552]\n",
      " [ 88 596]]\n",
      "BTCD Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.97      0.67       682\n",
      "        1.0       0.59      0.05      0.09       647\n",
      "\n",
      "avg / total       0.55      0.52      0.39      1329\n",
      "\n",
      "[[661  21]\n",
      " [617  30]]\n",
      "VRC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.53      0.52       651\n",
      "        1.0       0.54      0.52      0.53       678\n",
      "\n",
      "avg / total       0.53      0.53      0.53      1329\n",
      "\n",
      "[[345 306]\n",
      " [325 353]]\n",
      "LBC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.27      0.35       642\n",
      "        1.0       0.52      0.74      0.61       687\n",
      "\n",
      "avg / total       0.51      0.51      0.48      1329\n",
      "\n",
      "[[174 468]\n",
      " [179 508]]\n",
      "NEOS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.06      0.10       644\n",
      "        1.0       0.51      0.94      0.66       685\n",
      "\n",
      "avg / total       0.50      0.51      0.39      1329\n",
      "\n",
      "[[ 38 606]\n",
      " [ 42 643]]\n",
      "LTC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.68      0.55       606\n",
      "        1.0       0.57      0.35      0.43       723\n",
      "\n",
      "avg / total       0.52      0.50      0.49      1329\n",
      "\n",
      "[[411 195]\n",
      " [468 255]]\n",
      "VTC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       654\n",
      "        1.0       0.51      1.00      0.67       675\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1329\n",
      "\n",
      "[[  0 654]\n",
      " [  0 675]]\n",
      "OMNI Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.67      0.58       669\n",
      "        1.0       0.50      0.33      0.39       660\n",
      "\n",
      "avg / total       0.50      0.50      0.49      1329\n",
      "\n",
      "[[451 218]\n",
      " [445 215]]\n",
      "GRC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.58      0.19      0.28       679\n",
      "        1.0       0.50      0.86      0.63       650\n",
      "\n",
      "avg / total       0.54      0.51      0.45      1329\n",
      "\n",
      "[[127 552]\n",
      " [ 93 557]]\n",
      "EXP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.62      0.04      0.07       642\n",
      "        1.0       0.52      0.98      0.68       687\n",
      "\n",
      "avg / total       0.57      0.52      0.39      1329\n",
      "\n",
      "[[ 25 617]\n",
      " [ 15 672]]\n",
      "XEM Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.30      0.39       703\n",
      "        1.0       0.48      0.73      0.58       626\n",
      "\n",
      "avg / total       0.52      0.50      0.48      1329\n",
      "\n",
      "[[213 490]\n",
      " [172 454]]\n",
      "RADS Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.60      0.54       635\n",
      "        1.0       0.55      0.44      0.49       694\n",
      "\n",
      "avg / total       0.52      0.52      0.51      1329\n",
      "\n",
      "[[380 255]\n",
      " [388 306]]\n",
      "BURST Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.63      0.45      0.53       735\n",
      "        1.0       0.50      0.68      0.58       594\n",
      "\n",
      "avg / total       0.57      0.55      0.55      1329\n",
      "\n",
      "[[334 401]\n",
      " [192 402]]\n",
      "ETC Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.45      0.25      0.32       621\n",
      "        1.0       0.53      0.72      0.61       708\n",
      "\n",
      "avg / total       0.49      0.50      0.48      1329\n",
      "\n",
      "[[157 464]\n",
      " [195 513]]\n",
      "ARDR Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.20      0.29       667\n",
      "        1.0       0.50      0.81      0.62       662\n",
      "\n",
      "avg / total       0.51      0.51      0.46      1329\n",
      "\n",
      "[[135 532]\n",
      " [123 539]]\n",
      "BELA Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.01      0.02       669\n",
      "        1.0       0.50      0.99      0.66       660\n",
      "\n",
      "avg / total       0.48      0.50      0.34      1329\n",
      "\n",
      "[[  7 662]\n",
      " [  8 652]]\n",
      "AMP Logistic Regression\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.32      0.41       652\n",
      "        1.0       0.53      0.75      0.62       677\n",
      "\n",
      "avg / total       0.54      0.54      0.52      1329\n",
      "\n",
      "[[209 443]\n",
      " [170 507]]\n"
     ]
    }
   ],
   "source": [
    "for coin in colarr:\n",
    "    crypt = df[coin]\n",
    "    trainX = crypt['2015-01-01 00:00:00':'2017-01-01 00:00:00'].drop('up',axis=1)\n",
    "    trainY = crypt['2015-01-01 00:00:00':'2017-01-01 00:00:00']['up']\n",
    "    testX = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()].drop('up',axis=1)\n",
    "    testY = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()]['up']\n",
    "    \n",
    "    print coin + ' Logistic Regression'\n",
    "    \n",
    "    logmodel = LogisticRegression()\n",
    "    logmodel.fit(trainX,trainY)\n",
    "    pred = logmodel.predict(testX)\n",
    "    print classification_report(testY,pred)\n",
    "    print confusion_matrix(testY,pred)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOGE SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.66      1.00      0.80       882\n",
      "        1.0       0.00      0.00      0.00       447\n",
      "\n",
      "avg / total       0.44      0.66      0.53      1329\n",
      "\n",
      "[[882   0]\n",
      " [447   0]]\n",
      "NAV SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      1.00      0.65       638\n",
      "        1.0       0.00      0.00      0.00       691\n",
      "\n",
      "avg / total       0.23      0.48      0.31      1329\n",
      "\n",
      "[[638   0]\n",
      " [691   0]]\n",
      "NMC SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      1.00      0.65       634\n",
      "        1.0       0.00      0.00      0.00       695\n",
      "\n",
      "avg / total       0.23      0.48      0.31      1329\n",
      "\n",
      "[[634   0]\n",
      " [695   0]]\n",
      "NXT SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      1.00      0.66       649\n",
      "        1.0       0.00      0.00      0.00       680\n",
      "\n",
      "avg / total       0.24      0.49      0.32      1329\n",
      "\n",
      "[[649   0]\n",
      " [680   0]]\n",
      "DCR SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       625\n",
      "        1.0       0.50      1.00      0.67       625\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1250\n",
      "\n",
      "[[  0 625]\n",
      " [  0 625]]\n",
      "FLO SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      1.00      0.66       650\n",
      "        1.0       0.00      0.00      0.00       679\n",
      "\n",
      "avg / total       0.24      0.49      0.32      1329\n",
      "\n",
      "[[650   0]\n",
      " [679   0]]\n",
      "REP SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.00      0.00      0.00       654\n",
      "        1.0       0.51      1.00      0.67       675\n",
      "\n",
      "avg / total       0.26      0.51      0.34      1329\n",
      "\n",
      "[[  0 654]\n",
      " [  0 675]]\n",
      "BCY SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      1.00      0.66       662\n",
      "        1.0       0.00      0.00      0.00       667\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1329\n",
      "\n",
      "[[662   0]\n",
      " [667   0]]\n",
      "FCT SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      1.00      0.67       665\n",
      "        1.0       0.00      0.00      0.00       664\n",
      "\n",
      "avg / total       0.25      0.50      0.33      1329\n",
      "\n",
      "[[665   0]\n",
      " [664   0]]\n",
      "XRP SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      1.00      0.65       641\n",
      "        1.0       0.00      0.00      0.00       688\n",
      "\n",
      "avg / total       0.23      0.48      0.31      1329\n",
      "\n",
      "[[641   0]\n",
      " [688   0]]\n",
      "CLAM SVC\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e7778dd74fa5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/patrick/anaconda2/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/patrick/anaconda2/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for coin in colarr:\n",
    "    crypt = df[coin]\n",
    "    trainX = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00'].drop('up',axis=1)\n",
    "    trainY = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00']['up']\n",
    "    testX = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()].drop('up',axis=1)\n",
    "    testY = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()]['up']\n",
    "    \n",
    "    print coin + ' SVC'\n",
    "    \n",
    "    model = SVC()\n",
    "    model.fit(trainX,trainY)\n",
    "    pred = model.predict(testX)\n",
    "    print classification_report(testY,pred)\n",
    "    print confusion_matrix(testY,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOGE TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.69      0.46      0.55       882\n",
      "        1.0       0.36      0.60      0.45       447\n",
      "\n",
      "avg / total       0.58      0.50      0.51      1329\n",
      "\n",
      "[[403 479]\n",
      " [181 266]]\n",
      "NAV TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.45      0.47       638\n",
      "        1.0       0.53      0.57      0.55       691\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1329\n",
      "\n",
      "[[284 354]\n",
      " [294 397]]\n",
      "NMC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.46      0.47       634\n",
      "        1.0       0.52      0.54      0.53       695\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1329\n",
      "\n",
      "[[291 343]\n",
      " [318 377]]\n",
      "NXT TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.41      0.45       649\n",
      "        1.0       0.52      0.61      0.56       680\n",
      "\n",
      "avg / total       0.51      0.52      0.51      1329\n",
      "\n",
      "[[267 382]\n",
      " [262 418]]\n",
      "DCR TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.20      0.29       625\n",
      "        1.0       0.50      0.80      0.61       625\n",
      "\n",
      "avg / total       0.50      0.50      0.45      1250\n",
      "\n",
      "[[128 497]\n",
      " [128 497]]\n",
      "FLO TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.37      0.43       650\n",
      "        1.0       0.52      0.66      0.58       679\n",
      "\n",
      "avg / total       0.52      0.52      0.51      1329\n",
      "\n",
      "[[240 410]\n",
      " [229 450]]\n",
      "REP TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.51      0.51       654\n",
      "        1.0       0.52      0.52      0.52       675\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1329\n",
      "\n",
      "[[332 322]\n",
      " [323 352]]\n",
      "BCY TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.58      0.56       662\n",
      "        1.0       0.55      0.50      0.52       667\n",
      "\n",
      "avg / total       0.54      0.54      0.54      1329\n",
      "\n",
      "[[383 279]\n",
      " [332 335]]\n",
      "FCT TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.50      0.51       665\n",
      "        1.0       0.52      0.54      0.53       664\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1329\n",
      "\n",
      "[[333 332]\n",
      " [305 359]]\n",
      "XRP TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.49      0.50       641\n",
      "        1.0       0.53      0.55      0.54       688\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1329\n",
      "\n",
      "[[315 326]\n",
      " [313 375]]\n",
      "CLAM TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.52      0.51       663\n",
      "        1.0       0.50      0.48      0.49       666\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1329\n",
      "\n",
      "[[344 319]\n",
      " [347 319]]\n",
      "STRAT TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.55      0.55       668\n",
      "        1.0       0.54      0.54      0.54       661\n",
      "\n",
      "avg / total       0.54      0.54      0.54      1329\n",
      "\n",
      "[[368 300]\n",
      " [306 355]]\n",
      "NAUT TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.68      0.58       659\n",
      "        1.0       0.51      0.33      0.40       670\n",
      "\n",
      "avg / total       0.51      0.50      0.49      1329\n",
      "\n",
      "[[451 208]\n",
      " [451 219]]\n",
      "MAID TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.50      0.51       680\n",
      "        1.0       0.50      0.52      0.51       649\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1329\n",
      "\n",
      "[[338 342]\n",
      " [313 336]]\n",
      "PPC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.46      0.28      0.34       632\n",
      "        1.0       0.52      0.70      0.59       697\n",
      "\n",
      "avg / total       0.49      0.50      0.48      1329\n",
      "\n",
      "[[174 458]\n",
      " [208 489]]\n",
      "VIA TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.59      0.53       640\n",
      "        1.0       0.51      0.39      0.44       689\n",
      "\n",
      "avg / total       0.49      0.49      0.48      1329\n",
      "\n",
      "[[380 260]\n",
      " [419 270]]\n",
      "XCP TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.48      0.48       647\n",
      "        1.0       0.51      0.51      0.51       682\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1329\n",
      "\n",
      "[[312 335]\n",
      " [333 349]]\n",
      "XPM TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.52      0.50       648\n",
      "        1.0       0.51      0.47      0.49       681\n",
      "\n",
      "avg / total       0.49      0.49      0.49      1329\n",
      "\n",
      "[[334 314]\n",
      " [359 322]]\n",
      "ETH TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.44      0.47       670\n",
      "        1.0       0.50      0.58      0.54       659\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1329\n",
      "\n",
      "[[292 378]\n",
      " [276 383]]\n",
      "DASH TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.44      0.47       683\n",
      "        1.0       0.48      0.55      0.51       646\n",
      "\n",
      "avg / total       0.50      0.49      0.49      1329\n",
      "\n",
      "[[301 382]\n",
      " [291 355]]\n",
      "RIC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.48      0.49       636\n",
      "        1.0       0.54      0.56      0.55       693\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1329\n",
      "\n",
      "[[306 330]\n",
      " [303 390]]\n",
      "BTS TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.37      0.42       684\n",
      "        1.0       0.47      0.59      0.52       645\n",
      "\n",
      "avg / total       0.48      0.48      0.47      1329\n",
      "\n",
      "[[255 429]\n",
      " [265 380]]\n",
      "SYS TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.64      0.57       659\n",
      "        1.0       0.52      0.39      0.44       670\n",
      "\n",
      "avg / total       0.51      0.51      0.50      1329\n",
      "\n",
      "[[423 236]\n",
      " [412 258]]\n",
      "NOTE TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.58      0.55       685\n",
      "        1.0       0.48      0.41      0.44       644\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1329\n",
      "\n",
      "[[399 286]\n",
      " [378 266]]\n",
      "FLDC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.63      0.30      0.41       726\n",
      "        1.0       0.48      0.79      0.60       603\n",
      "\n",
      "avg / total       0.57      0.52      0.50      1329\n",
      "\n",
      "[[218 508]\n",
      " [126 477]]\n",
      "EMC2 TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.60      0.56      0.58       776\n",
      "        1.0       0.44      0.48      0.46       553\n",
      "\n",
      "avg / total       0.54      0.53      0.53      1329\n",
      "\n",
      "[[434 342]\n",
      " [285 268]]\n",
      "SC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.58      0.38      0.45       780\n",
      "        1.0       0.41      0.61      0.49       549\n",
      "\n",
      "avg / total       0.51      0.47      0.47      1329\n",
      "\n",
      "[[293 487]\n",
      " [215 334]]\n",
      "PINK TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.48      0.50       716\n",
      "        1.0       0.44      0.47      0.46       613\n",
      "\n",
      "avg / total       0.48      0.48      0.48      1329\n",
      "\n",
      "[[343 373]\n",
      " [322 291]]\n",
      "XMR TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.49      0.51       653\n",
      "        1.0       0.54      0.58      0.56       676\n",
      "\n",
      "avg / total       0.53      0.53      0.53      1329\n",
      "\n",
      "[[317 336]\n",
      " [282 394]]\n",
      "HUC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.18      0.27       634\n",
      "        1.0       0.53      0.83      0.64       695\n",
      "\n",
      "avg / total       0.51      0.52      0.46      1329\n",
      "\n",
      "[[117 517]\n",
      " [121 574]]\n",
      "SJCX TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.46      0.39      0.42       646\n",
      "        1.0       0.50      0.58      0.54       683\n",
      "\n",
      "avg / total       0.48      0.48      0.48      1329\n",
      "\n",
      "[[249 397]\n",
      " [288 395]]\n",
      "XBC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.48      0.37      0.42       657\n",
      "        1.0       0.50      0.61      0.54       672\n",
      "\n",
      "avg / total       0.49      0.49      0.48      1329\n",
      "\n",
      "[[242 415]\n",
      " [265 407]]\n",
      "STR TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.55      0.45      0.49       691\n",
      "        1.0       0.50      0.60      0.55       638\n",
      "\n",
      "avg / total       0.53      0.52      0.52      1329\n",
      "\n",
      "[[308 383]\n",
      " [254 384]]\n",
      "DGB TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.66      0.58      0.62       818\n",
      "        1.0       0.43      0.51      0.47       511\n",
      "\n",
      "avg / total       0.57      0.56      0.56      1329\n",
      "\n",
      "[[476 342]\n",
      " [249 262]]\n",
      "BLK TREE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.45      0.48       666\n",
      "        1.0       0.51      0.57      0.53       663\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1329\n",
      "\n",
      "[[301 365]\n",
      " [288 375]]\n",
      "ZEC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.46      0.47      0.46       594\n",
      "        1.0       0.56      0.55      0.55       735\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1329\n",
      "\n",
      "[[277 317]\n",
      " [331 404]]\n",
      "NXC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.18      0.27       654\n",
      "        1.0       0.51      0.84      0.64       675\n",
      "\n",
      "avg / total       0.52      0.52      0.46      1329\n",
      "\n",
      "[[117 537]\n",
      " [105 570]]\n",
      "POT TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.54      0.42      0.47       663\n",
      "        1.0       0.53      0.64      0.58       666\n",
      "\n",
      "avg / total       0.53      0.53      0.52      1329\n",
      "\n",
      "[[277 386]\n",
      " [238 428]]\n",
      "LSK TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.44      0.46       645\n",
      "        1.0       0.52      0.57      0.54       684\n",
      "\n",
      "avg / total       0.50      0.51      0.50      1329\n",
      "\n",
      "[[283 362]\n",
      " [295 389]]\n",
      "GAME TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.49      0.61      0.54       645\n",
      "        1.0       0.53      0.41      0.46       684\n",
      "\n",
      "avg / total       0.51      0.51      0.50      1329\n",
      "\n",
      "[[391 254]\n",
      " [401 283]]\n",
      "BTCD TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.43      0.47       682\n",
      "        1.0       0.49      0.58      0.53       647\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1329\n",
      "\n",
      "[[291 391]\n",
      " [272 375]]\n",
      "VRC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.51      0.51       651\n",
      "        1.0       0.52      0.52      0.52       678\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1329\n",
      "\n",
      "[[333 318]\n",
      " [327 351]]\n",
      "LBC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.52      0.51       642\n",
      "        1.0       0.54      0.52      0.53       687\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1329\n",
      "\n",
      "[[334 308]\n",
      " [331 356]]\n",
      "NEOS TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.54      0.53       644\n",
      "        1.0       0.55      0.53      0.54       685\n",
      "\n",
      "avg / total       0.53      0.53      0.53      1329\n",
      "\n",
      "[[345 299]\n",
      " [325 360]]\n",
      "LTC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.46      0.52      0.49       606\n",
      "        1.0       0.55      0.49      0.52       723\n",
      "\n",
      "avg / total       0.51      0.50      0.50      1329\n",
      "\n",
      "[[318 288]\n",
      " [372 351]]\n",
      "VTC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.31      0.37       654\n",
      "        1.0       0.49      0.65      0.56       675\n",
      "\n",
      "avg / total       0.48      0.49      0.47      1329\n",
      "\n",
      "[[205 449]\n",
      " [235 440]]\n",
      "OMNI TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.52      0.47      0.50       669\n",
      "        1.0       0.51      0.56      0.54       660\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1329\n",
      "\n",
      "[[317 352]\n",
      " [288 372]]\n",
      "GRC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.47      0.50       679\n",
      "        1.0       0.51      0.57      0.53       650\n",
      "\n",
      "avg / total       0.52      0.52      0.52      1329\n",
      "\n",
      "[[318 361]\n",
      " [281 369]]\n",
      "EXP TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.38      0.44       642\n",
      "        1.0       0.54      0.68      0.60       687\n",
      "\n",
      "avg / total       0.53      0.53      0.52      1329\n",
      "\n",
      "[[246 396]\n",
      " [222 465]]\n",
      "XEM TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.53      0.44      0.48       703\n",
      "        1.0       0.47      0.57      0.51       626\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1329\n",
      "\n",
      "[[307 396]\n",
      " [272 354]]\n",
      "RADS TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.56      0.53       635\n",
      "        1.0       0.56      0.51      0.53       694\n",
      "\n",
      "avg / total       0.53      0.53      0.53      1329\n",
      "\n",
      "[[354 281]\n",
      " [340 354]]\n",
      "BURST TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.59      0.45      0.51       735\n",
      "        1.0       0.47      0.61      0.53       594\n",
      "\n",
      "avg / total       0.54      0.52      0.52      1329\n",
      "\n",
      "[[333 402]\n",
      " [234 360]]\n",
      "ETC TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.47      0.49      0.48       621\n",
      "        1.0       0.53      0.50      0.52       708\n",
      "\n",
      "avg / total       0.50      0.50      0.50      1329\n",
      "\n",
      "[[307 314]\n",
      " [351 357]]\n",
      "ARDR TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.24      0.32       667\n",
      "        1.0       0.50      0.76      0.60       662\n",
      "\n",
      "avg / total       0.50      0.50      0.46      1329\n",
      "\n",
      "[[160 507]\n",
      " [160 502]]\n",
      "BELA TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.51      0.28      0.36       669\n",
      "        1.0       0.50      0.73      0.60       660\n",
      "\n",
      "avg / total       0.51      0.50      0.48      1329\n",
      "\n",
      "[[184 485]\n",
      " [175 485]]\n",
      "AMP TREE\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.50      0.45      0.47       652\n",
      "        1.0       0.52      0.57      0.54       677\n",
      "\n",
      "avg / total       0.51      0.51      0.51      1329\n",
      "\n",
      "[[291 361]\n",
      " [290 387]]\n"
     ]
    }
   ],
   "source": [
    "for coin in colarr:\n",
    "    crypt = df[coin]\n",
    "    trainX = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00'].drop('up',axis=1)\n",
    "    trainY = crypt[crypt['open'].first_valid_index():'2017-01-01 00:00:00']['up']\n",
    "    testX = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()].drop('up',axis=1)\n",
    "    testY = crypt['2017-01-01 04:00:00':crypt['up'].last_valid_index()]['up']\n",
    "    \n",
    "    print coin + ' TREE'\n",
    "    \n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(trainX,trainY)\n",
    "    pred = model.predict(testX)\n",
    "    print classification_report(testY,pred)\n",
    "    print confusion_matrix(testY,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
